{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중언어 개체명인식\n",
    "\n",
    "- 제로샷 교차 언어 전이 (zero-shot cross lingual switching) 가능\n",
    "  - 한 언어에서 파인 튜닝된 모델이 훈련없이 다른 모델에 적용 가능하다.\n",
    "- 코드 스위칭 에 적합\n",
    "  - 하나의 대화에서 둘이상의 언어나, 사투리등을 바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XTREME 데이터 셋 이용\n",
    "    - PAN-X, wikiANN : 교차 언어 데이터 셋\n",
    "    - LOC, PER, ORG\n",
    "    - IOB2 포맷\n",
    "        - i- 동일 개체명의 연속\n",
    "        - o- 어떤 개체에도 속하지 않음\n",
    "        - B- 개체명의 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('서브셋 갯수', 183)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "\"서브셋 갯수\",len(xtreme_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af: PAN-X.af, ar: PAN-X.ar, \n",
      "bg: PAN-X.bg, bn: PAN-X.bn, de: PAN-X.de, el: PAN-X.el, en: PAN-X.en, \n",
      "es: PAN-X.es, et: PAN-X.et, eu: PAN-X.eu, fa: PAN-X.fa, fi: PAN-X.fi, \n",
      "fr: PAN-X.fr, he: PAN-X.he, hi: PAN-X.hi, hu: PAN-X.hu, id: PAN-X.id, \n",
      "it: PAN-X.it, ja: PAN-X.ja, jv: PAN-X.jv, ka: PAN-X.ka, kk: PAN-X.kk, \n",
      "ko: PAN-X.ko, ml: PAN-X.ml, mr: PAN-X.mr, ms: PAN-X.ms, my: PAN-X.my, \n",
      "nl: PAN-X.nl, pt: PAN-X.pt, ru: PAN-X.ru, sw: PAN-X.sw, ta: PAN-X.ta, \n",
      "te: PAN-X.te, th: PAN-X.th, tl: PAN-X.tl, tr: PAN-X.tr, ur: PAN-X.ur, \n",
      "vi: PAN-X.vi, yo: PAN-X.yo, zh: PAN-X.zh, "
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(xtreme_subsets):\n",
    "    if \"PAN\" in l:\n",
    "        print(l.split(\".\")[-1], end=': ')\n",
    "        print(l, end=', ')\n",
    "        if i % 5 == 0:\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언어 선택하기\n",
    "\n",
    "- 선택 언어\n",
    "    - PAN-X.en : 영어\n",
    "    - PAN-X.ko : 한국어\n",
    "    - PAN-X.ja : 일본어\n",
    "    - PAN-X.es : 스페인어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "#일반적인 불균형 상태 만들기\n",
    "langs = [\"en\", \"ko\", \"ja\", \"es\"]\n",
    "fracs = [0.12, 0.6, 0.8, 0.1]\n",
    "\n",
    "panx_ch = defaultdict(DatasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang, frac in zip(langs, fracs):\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split]=(\n",
    "            ds[split]\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(int(frac*ds[split].num_rows))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "      <th>ja</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Samples</th>\n",
       "      <td>2400</td>\n",
       "      <td>12000</td>\n",
       "      <td>16000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en     ko     ja    es\n",
       "Samples  2400  12000  16000  2000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang : [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['《트와일라잇》을', '같이', '찍은', '에디', '가테지', ',', '크리스틴', '스튜어트', ',', '로버트', '패틴슨과는', '매우', '친한사이라고', '한다', '.']\n",
      "ner_tags [0, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0]\n",
      "langs ['ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko']\n",
      "\n",
      "features 중 ner-태그 확인\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = panx_ch[\"ko\"][\"train\"][0]\n",
    "\n",
    "for k, v in element.items():\n",
    "    print(k,v)\n",
    "\n",
    "print(\"\\nfeatures 중 ner-태그 확인\")\n",
    "ner_tags = panx_ch[\"ko\"][\"train\"].features[\"ner_tags\"].feature\n",
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tag_names'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tag_names'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tag_names'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER-태그를 인덱스 에서 태그명 변경 및 추가\n",
    "\n",
    "panx_ko = panx_ch[\"ko\"].map(lambda x : {\"ner_tag_names\" :\n",
    "                                        [ner_tags.int2str(idx)\n",
    "                                        for idx in x[\"ner_tags\"]]})\n",
    "panx_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>《트와일라잇》을</td>\n",
       "      <td>같이</td>\n",
       "      <td>찍은</td>\n",
       "      <td>에디</td>\n",
       "      <td>가테지</td>\n",
       "      <td>,</td>\n",
       "      <td>크리스틴</td>\n",
       "      <td>스튜어트</td>\n",
       "      <td>,</td>\n",
       "      <td>로버트</td>\n",
       "      <td>패틴슨과는</td>\n",
       "      <td>매우</td>\n",
       "      <td>친한사이라고</td>\n",
       "      <td>한다</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tag_name</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0   1   2      3      4  5      6      7  8      9   \\\n",
       "tokens        《트와일라잇》을  같이  찍은     에디    가테지  ,   크리스틴   스튜어트  ,    로버트   \n",
       "ner_tag_name         O   O   O  B-PER  I-PER  O  B-PER  I-PER  O  B-PER   \n",
       "\n",
       "                 10  11      12  13 14  \n",
       "tokens        패틴슨과는  매우  친한사이라고  한다  .  \n",
       "ner_tag_name  I-PER   O       O   O  O  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([panx_ko[\"train\"][0][\"tokens\"],\n",
    "              panx_ko[\"train\"][0][\"ner_tag_names\"]],\n",
    "              index=[\"tokens\",\"ner_tag_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과: 사람 이름에 태그가 붙었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그 분포 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'train': Counter({'LOC': 7097, 'ORG': 5361, 'PER': 4870}),\n",
       "             'validation': Counter({'LOC': 3579, 'ORG': 2755, 'PER': 2448}),\n",
       "             'test': Counter({'LOC': 3503, 'ORG': 2584, 'PER': 2557})})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "\n",
    "for split , dataset in panx_ko.items():\n",
    "    for row in dataset[\"ner_tag_names\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "\n",
    "split2freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 언어 트랜스 포머"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반 적으로 데이터셋에 여러 언어가 있어도 일반화가 가능하다.\n",
    "- 다중 언어 트랜스 포머 평가 방법\n",
    "    1. **en (영어 훈련 후 평가)**: 영어 데이터로 훈련한 후 다른 언어에서 평가.\n",
    "    2. **each (단일 훈련 및 단일 평가)**: 각 언어별로 별도로 훈련하고 평가.\n",
    "    3. **all (모든 훈련셋에서 평가)**: 모든 언어 데이터를 사용해 훈련하고 각 언어에서 평가."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R(Cross-lingual LM)\n",
    "- RoBERTa 의 다중 언어 모델 버젼 : XLM-RoBERTa\n",
    "- 100개 의 언어로 훈련\n",
    "\n",
    "- **SentencePiece** 토크나이저\n",
    "    - Unigram(부분 단어 분할 방식) 기반의 인코딩 방식을 이용\n",
    "    - 특정 언어에 대한 지식없이 텍스트 처리 가능\n",
    "    - 다국어 말뭉치에 유용하다\n",
    "    - 다양한 언어 모델과 호환성이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bertmodel = \"bert-base-cased\"\n",
    "xlmrmodel = \"xlm-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bertmodel)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmrmodel) # Sentence Piece 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'I', '##m', 'Your', 'Father', '[SEP]']\n",
      "['<s>', '▁Im', '▁Your', '▁Father', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text_test = \"Im Your Father\"\n",
    "\n",
    "print(bert_tokenizer.convert_ids_to_tokens(bert_tokenizer(text_test).input_ids))\n",
    "print(xlmr_tokenizer.convert_ids_to_tokens(xlmr_tokenizer(text_test).input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 파이프라인\n",
    "\n",
    "1. **정규화 (Normalization)**\n",
    "   - 텍스트를 일관된 형태로 변환.\n",
    "   - 예: `'Im Your Father'` → `'im your father'`\n",
    "\n",
    "2. **사전 토큰화 (Pre-tokenization)**\n",
    "   - 공백과 구두점을 기준으로 단어로 나눈다..\n",
    "   - 예: `'im your father'` → `['im', 'your', 'father']`\n",
    "\n",
    "3. **토크나이저 모델 (Tokenizer Model)**\n",
    "   - 단어를 고유한 숫자 ID로 바꿈.\n",
    "   - 예: `['im', 'your', 'father']` → `[125, 52, 482]`\n",
    "\n",
    "4. **사후처리 (Post-processing)**\n",
    "   - 시작과 끝을 나타내는 특수 토큰을 추가.\n",
    "   - 예: `[CLS] im your father [SEP]` → `[0, 125, 52, 482, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스 포머 모델 클래스의 형식\n",
    "![modelfortask](images/04_01.png)\n",
    "- `<ModelName>For<Task>` 형식을 띔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaXLConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaXLConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # 설정 초기화\n",
    "        self.num_labels =  config.num_labels\n",
    "        \n",
    "        # 모델 바디 로드\n",
    "        # add_pooling_layer=False : 모든 히든 스테이트 반환\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "\n",
    "        # 분류헤드\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # 가중치 로드 및 초기화\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids = None, attention_mask = None,\n",
    "                token_type_ids = None, labels = None, **kwargs):\n",
    "        # 모델 바디 순전파 결과\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               **kwargs)\n",
    "        \n",
    "        # 분류 헤드 순전파\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        # 손실값 계산\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            loss = loss_function(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # 객체 출력\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그 이름 인덱스 딕셔너리 생성\n",
    "index2tag = {idx: tag for idx, tag in enumerate(ner_tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(ner_tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig # 모델 구조의 설계도,\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmrmodel,\n",
    "                                         num_labels = ner_tags.num_classes,\n",
    "                                         id2label = index2tag,\n",
    "                                         label2id = tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "xlmr_model= (XLMRobertaForTokenClassification\n",
    "               .from_pretrained(xlmrmodel, config=xlmr_config)\n",
    "               .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>0</td>\n",
       "      <td>3370</td>\n",
       "      <td>14804</td>\n",
       "      <td>160960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Im</td>\n",
       "      <td>Your</td>\n",
       "      <td>Father</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1      2       3     4\n",
       "idx      0  3370  14804  160960     2\n",
       "token  <s>    Im   Your  Father  </s>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text_test, return_tensors=\"pt\")\n",
    "result_pd = pd.DataFrame([[int(i) for i in input_ids[0]],\n",
    "              [xlmr_tokenizer.decode(i) for i in input_ids[0]]],\n",
    "              index=[\"idx\",\"token\"])\n",
    "result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd.loc[\"result\"]=[index2tag[int(i)] for i in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>0</td>\n",
       "      <td>3370</td>\n",
       "      <td>14804</td>\n",
       "      <td>160960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Im</td>\n",
       "      <td>Your</td>\n",
       "      <td>Father</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2       3      4\n",
       "idx         0   3370  14804  160960      2\n",
       "token     <s>     Im   Your  Father   </s>\n",
       "result  I-LOC  I-LOC  I-LOC   I-LOC  I-LOC"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pd # 아직 훈련전 이기 때문에 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁transform</td>\n",
       "      <td>er</td>\n",
       "      <td>▁is</td>\n",
       "      <td>▁fun</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ids</th>\n",
       "      <td>0</td>\n",
       "      <td>27198</td>\n",
       "      <td>56</td>\n",
       "      <td>83</td>\n",
       "      <td>7477</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1      2      3      4      5      6\n",
       "Token    <s>  ▁transform     er    ▁is   ▁fun      !   </s>\n",
       "Ids        0       27198     56     83   7477     38      2\n",
       "Tags   I-LOC       I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 이후 작업을 위하여 데이터 프레임 만들어주는 함수 생성\n",
    "\n",
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # 텍스트 토크나이징\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # seq 인코딩\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    pred_str = [tags[p] for p in predictions[0]]\n",
    "    return pd.DataFrame([tokens,\n",
    "                         input_ids.cpu().numpy()[0],\n",
    "                         pred_str], index=[\"Token\",\"Ids\",\"Tags\"]\n",
    "                        )\n",
    "\n",
    "\n",
    "tag_text(\"transformer is fun!\",\n",
    "         ner_tags.names,\n",
    "         xlmr_model,\n",
    "         xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tokens = panx_ko['train']['tokens'], panx_ko['train']['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=44, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>0</td>\n",
       "      <td>953</td>\n",
       "      <td>46349</td>\n",
       "      <td>19050</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1388</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_ids</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁19</td>\n",
       "      <td>개의</td>\n",
       "      <td>▁지역</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offsets</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>special_tokens_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overflowing</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0       1       2       3       4       5       6  \\\n",
       "input_ids                 0     953   46349   19050      15       6       4   \n",
       "type_ids                  0       0       0       0       0       0       0   \n",
       "tokens                  <s>     ▁19      개의     ▁지역      ▁(       ▁       ,   \n",
       "offsets              (0, 0)  (0, 2)  (2, 4)  (0, 2)  (0, 1)  (0, 1)  (0, 1)   \n",
       "attention_mask            1       1       1       1       1       1       1   \n",
       "special_tokens_mask       1       0       0       0       0       0       0   \n",
       "overflowing            None    None    None    None    None    None    None   \n",
       "\n",
       "                          7       8  \n",
       "input_ids              1388       2  \n",
       "type_ids                  0       0  \n",
       "tokens                   ▁)    </s>  \n",
       "offsets              (0, 1)  (0, 0)  \n",
       "attention_mask            1       1  \n",
       "special_tokens_mask       0       1  \n",
       "overflowing            None    None  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 16\n",
    "pd.DataFrame([tokenized_input[i].ids,\n",
    "              tokenized_input[i].type_ids,\n",
    "              tokenized_input[i].tokens,\n",
    "              tokenized_input[i].offsets,\n",
    "              tokenized_input[i].attention_mask,\n",
    "              tokenized_input[i].special_tokens_mask,\n",
    "              tokenized_input[i].overflowing],\n",
    "              index=[\"input_ids\", \"type_ids\", \"tokens\", \"offsets\", \"attention_mask\", \"special_tokens_mask\", \"overflowing\", ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-RoBERTa 파인 튜닝 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea532a3151e4e5ba7a23eb22ee93da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f2d3875e974c439d6b1ef8dcb41358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokens_to_input_ids(batch):\n",
    "    return xlmr_tokenizer(batch['tokens'],\n",
    "                          is_split_into_words=True, \n",
    "                          padding=True, \n",
    "                          truncation=True, \n",
    "                          return_tensors=\"pt\")\n",
    "\n",
    "# map 함수를 사용하여 데이터 변환\n",
    "ko_train = panx_ko['train'].map(tokens_to_input_ids, batched=True, batch_size=12)\n",
    "ko_valid = panx_ko['validation'].map(tokens_to_input_ids, batched=True, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "n_epoch = 3\n",
    "b_size_ = 24\n",
    "logging_steps = len(panx_ko['train']) // b_size_\n",
    "model_name = f\"{xlmrmodel}-finetuned-panx-kr\"\n",
    "train_arg = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=n_epoch,\n",
    "    per_device_eval_batch_size=b_size_,\n",
    "    per_device_train_batch_size=b_size_,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3bad3959fc4d43b44d2119d2a58164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 예측 정리\n",
    "def align_preds(preds, label_ids):\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list , preds_list = [], []\n",
    "\n",
    "    for batch_i in range(batch_size):\n",
    "        example_labels, example_preds =[], []\n",
    "        for seq_i in range(seq_len):\n",
    "            if label_ids[batch_i,seq_i] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_i][seq_i]])\n",
    "                example_preds.append(index2tag[preds[batch_i][seq_i]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "# 평가 결과\n",
    "def result_metrics(eval_pred):\n",
    "    y_pred, y_true = align_preds(eval_pred.predictions,\n",
    "                                 eval_pred.label_ids)\n",
    "    return {\"f1\" : f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 콜레이터 : 토큰 분류 및 시퀀스 길이 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# xlmr 토크나이저를 이용하여 패딩 및 토크나이징\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 재사용을 위함 함수 정의\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(xlmrmodel, config=xlmr_config).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
