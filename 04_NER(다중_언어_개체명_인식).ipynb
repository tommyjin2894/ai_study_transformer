{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중언어 개체명인식\n",
    "\n",
    "- 제로샷 교차 언어 전이 (zero-shot cross lingual switching) 가능\n",
    "  - 한 언어에서 파인 튜닝된 모델이 훈련없이 다른 모델에 적용 가능하다.\n",
    "- 코드 스위칭 에 적합\n",
    "  - 하나의 대화에서 둘이상의 언어나, 사투리등을 바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XTREME 데이터 셋 이용\n",
    "    - PAN-X, wikiANN : 교차 언어 데이터 셋\n",
    "    - LOC, PER, ORG\n",
    "    - IOB2 포맷\n",
    "        - i- 동일 개체명의 연속\n",
    "        - o- 어떤 개체에도 속하지 않음\n",
    "        - B- 개체명의 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('서브셋 갯수', 183)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "\"서브셋 갯수\",len(xtreme_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af: PAN-X.af, ar: PAN-X.ar, \n",
      "bg: PAN-X.bg, bn: PAN-X.bn, de: PAN-X.de, el: PAN-X.el, en: PAN-X.en, \n",
      "es: PAN-X.es, et: PAN-X.et, eu: PAN-X.eu, fa: PAN-X.fa, fi: PAN-X.fi, \n",
      "fr: PAN-X.fr, he: PAN-X.he, hi: PAN-X.hi, hu: PAN-X.hu, id: PAN-X.id, \n",
      "it: PAN-X.it, ja: PAN-X.ja, jv: PAN-X.jv, ka: PAN-X.ka, kk: PAN-X.kk, \n",
      "ko: PAN-X.ko, ml: PAN-X.ml, mr: PAN-X.mr, ms: PAN-X.ms, my: PAN-X.my, \n",
      "nl: PAN-X.nl, pt: PAN-X.pt, ru: PAN-X.ru, sw: PAN-X.sw, ta: PAN-X.ta, \n",
      "te: PAN-X.te, th: PAN-X.th, tl: PAN-X.tl, tr: PAN-X.tr, ur: PAN-X.ur, \n",
      "vi: PAN-X.vi, yo: PAN-X.yo, zh: PAN-X.zh, "
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(xtreme_subsets):\n",
    "    if \"PAN\" in l:\n",
    "        print(l.split(\".\")[-1], end=': ')\n",
    "        print(l, end=', ')\n",
    "        if i % 5 == 0:\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언어 선택하기\n",
    "\n",
    "- 선택 언어\n",
    "    - PAN-X.en : 영어\n",
    "    - PAN-X.ko : 한국어\n",
    "    - PAN-X.ja : 일본어\n",
    "    - PAN-X.es : 스페인어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "#일반적인 불균형 상태 만들기\n",
    "langs = [\"en\", \"ko\", \"ja\", \"es\"]\n",
    "fracs = [0.12, 0.6, 0.8, 0.1]\n",
    "\n",
    "panx_ch = defaultdict(DatasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang, frac in zip(langs, fracs):\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split]=(\n",
    "            ds[split]\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(int(frac*ds[split].num_rows))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "      <th>ja</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Samples</th>\n",
       "      <td>2400</td>\n",
       "      <td>12000</td>\n",
       "      <td>16000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en     ko     ja    es\n",
       "Samples  2400  12000  16000  2000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang : [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['《트와일라잇》을', '같이', '찍은', '에디', '가테지', ',', '크리스틴', '스튜어트', ',', '로버트', '패틴슨과는', '매우', '친한사이라고', '한다', '.']\n",
      "ner_tags [0, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0]\n",
      "langs ['ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko', 'ko']\n",
      "\n",
      "features 중 ner-태그 확인\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = panx_ch[\"ko\"][\"train\"][0]\n",
    "\n",
    "for k, v in element.items():\n",
    "    print(k,v)\n",
    "\n",
    "print(\"\\nfeatures 중 ner-태그 확인\")\n",
    "ner_tags = panx_ch[\"ko\"][\"train\"].features[\"ner_tags\"].feature\n",
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tag_names'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tag_names'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tag_names'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER-태그를 인덱스 에서 태그명 변경 및 추가\n",
    "\n",
    "panx_ko = panx_ch[\"ko\"].map(lambda x : {\"ner_tag_names\" :\n",
    "                                        [ner_tags.int2str(idx)\n",
    "                                        for idx in x[\"ner_tags\"]]})\n",
    "panx_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>《트와일라잇》을</td>\n",
       "      <td>같이</td>\n",
       "      <td>찍은</td>\n",
       "      <td>에디</td>\n",
       "      <td>가테지</td>\n",
       "      <td>,</td>\n",
       "      <td>크리스틴</td>\n",
       "      <td>스튜어트</td>\n",
       "      <td>,</td>\n",
       "      <td>로버트</td>\n",
       "      <td>패틴슨과는</td>\n",
       "      <td>매우</td>\n",
       "      <td>친한사이라고</td>\n",
       "      <td>한다</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tag_name</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0   1   2      3      4  5      6      7  8      9   \\\n",
       "tokens        《트와일라잇》을  같이  찍은     에디    가테지  ,   크리스틴   스튜어트  ,    로버트   \n",
       "ner_tag_name         O   O   O  B-PER  I-PER  O  B-PER  I-PER  O  B-PER   \n",
       "\n",
       "                 10  11      12  13 14  \n",
       "tokens        패틴슨과는  매우  친한사이라고  한다  .  \n",
       "ner_tag_name  I-PER   O       O   O  O  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([panx_ko[\"train\"][0][\"tokens\"],\n",
    "              panx_ko[\"train\"][0][\"ner_tag_names\"]],\n",
    "              index=[\"tokens\",\"ner_tag_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과: 사람 이름에 태그가 붙었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그 분포 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'train': Counter({'LOC': 7097, 'ORG': 5361, 'PER': 4870}),\n",
       "             'validation': Counter({'LOC': 3579, 'ORG': 2755, 'PER': 2448}),\n",
       "             'test': Counter({'LOC': 3503, 'ORG': 2584, 'PER': 2557})})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "\n",
    "for split , dataset in panx_ko.items():\n",
    "    for row in dataset[\"ner_tag_names\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "\n",
    "split2freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 언어 트랜스 포머"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반 적으로 데이터셋에 여러 언어가 있어도 일반화가 가능하다.\n",
    "- 다중 언어 트랜스 포머 평가 방법\n",
    "    1. **en (영어 훈련 후 평가)**: 영어 데이터로 훈련한 후 다른 언어에서 평가.\n",
    "    2. **each (단일 훈련 및 단일 평가)**: 각 언어별로 별도로 훈련하고 평가.\n",
    "    3. **all (모든 훈련셋에서 평가)**: 모든 언어 데이터를 사용해 훈련하고 각 언어에서 평가."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R(Cross-lingual LM)\n",
    "- RoBERTa 의 다중 언어 모델 버젼 : XLM-RoBERTa\n",
    "- 100개 의 언어로 훈련\n",
    "\n",
    "- **SentencePiece** 토크나이저\n",
    "    - Unigram(부분 단어 분할 방식) 기반의 인코딩 방식을 이용\n",
    "    - 특정 언어에 대한 지식없이 텍스트 처리 가능\n",
    "    - 다국어 말뭉치에 유용하다\n",
    "    - 다양한 언어 모델과 호환성이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bertmodel = \"bert-base-cased\"\n",
    "xlmrmodel = \"xlm-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bertmodel)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmrmodel) # Sentence Piece 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'I', '##m', 'Your', 'Father', '[SEP]']\n",
      "['<s>', '▁Im', '▁Your', '▁Father', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text_test = \"Im Your Father\"\n",
    "\n",
    "print(bert_tokenizer.convert_ids_to_tokens(bert_tokenizer(text_test).input_ids))\n",
    "print(xlmr_tokenizer.convert_ids_to_tokens(xlmr_tokenizer(text_test).input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 파이프라인\n",
    "\n",
    "1. **정규화 (Normalization)**\n",
    "   - 텍스트를 일관된 형태로 변환.\n",
    "   - 예: `'Im Your Father'` → `'im your father'`\n",
    "\n",
    "2. **사전 토큰화 (Pre-tokenization)**\n",
    "   - 공백과 구두점을 기준으로 단어로 나눈다..\n",
    "   - 예: `'im your father'` → `['im', 'your', 'father']`\n",
    "\n",
    "3. **토크나이저 모델 (Tokenizer Model)**\n",
    "   - 단어를 고유한 숫자 ID로 바꿈.\n",
    "   - 예: `['im', 'your', 'father']` → `[125, 52, 482]`\n",
    "\n",
    "4. **사후처리 (Post-processing)**\n",
    "   - 시작과 끝을 나타내는 특수 토큰을 추가.\n",
    "   - 예: `[CLS] im your father [SEP]` → `[0, 125, 52, 482, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스 포머 모델 클래스의 형식\n",
    "![modelfortask](images/04_01.png)\n",
    "- `<ModelName>For<Task>` 형식을 띔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xtreme 데이터 셋 -> 커널 종료 후 아래 코드 부터 다시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.ko', 'PAN-X.en', 'PAN-X.ja', 'PAN-X.es']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "# xtreme 데이터셋 이름 확인\n",
    "subsets = get_dataset_config_names(\"xtreme\")\n",
    "\n",
    "def find_lan(lan):\n",
    "    return [j for j in [i for i in subsets if \"PAN-X\" in i]\n",
    "                if lan in j][0]\n",
    "\n",
    "language_names = [find_lan(l) for l in [\"ko\", \"en\", \"ja\", \"es\"]]\n",
    "language_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 셋 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "down_sample_ratios = [0.5, 0.3, 0.2, 0.1] #현실 데이터 와 같이 데이터 불균형 만들기\n",
    "# 데이터 셋 로드\n",
    "\n",
    "for l, down_sample_ratio in zip(language_names, down_sample_ratios):\n",
    "    l_name = (l.split('.')[-1])\n",
    "    down_sample = load_dataset(\"xtreme\", name=l)\n",
    "\n",
    "    for split in down_sample:\n",
    "        # 선택할 데이터 포인트의 수\n",
    "        num_datas = int(down_sample[split].num_rows * down_sample_ratio)\n",
    "        panx_ch[l_name][split] = down_sample[split].shuffle(seed=42).select(range(num_datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 언어별 데이터 포인트 수 :\n",
      "[('ko', {'train': 10000, 'validation': 5000, 'test': 5000}), ('en', {'train': 6000, 'validation': 3000, 'test': 3000}), ('ja', {'train': 4000, 'validation': 2000, 'test': 2000}), ('es', {'train': 2000, 'validation': 1000, 'test': 1000})]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"각 언어별 데이터 포인트 수 :\n",
    "{[(j,panx_ch[j].num_rows) for j in [i for i in panx_ch]]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'} \n",
      " {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "labels = panx_ch[\"ko\"][\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "i2l = {k:v for k,v in enumerate(labels)}\n",
    "l2i = {v:k for k,v in enumerate(labels)}\n",
    "\n",
    "print(i2l,\"\\n\",l2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라벨 맵핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags_str = lambda bs : {\"ner_tags_str\" :([[i2l[i] for i in b] for b in bs[\"ner_tags\"]])}\n",
    "\n",
    "for l in [\"ko\", \"en\", \"ja\", \"es\"]:\n",
    "    panx_ch[l] = (panx_ch[l]\n",
    "                    .map(ner_tags_str,\n",
    "                    batch_size= 100, batched=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>Jake</td>\n",
       "      <td>McGee</td>\n",
       "      <td>,</td>\n",
       "      <td>current</td>\n",
       "      <td>MLB</td>\n",
       "      <td>player</td>\n",
       "      <td>(</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>Bay</td>\n",
       "      <td>Rays</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tags</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>langs</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tags_str</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0      1   2        3      4       5   6      7      8   \\\n",
       "tokens         Jake  McGee   ,  current    MLB  player   (  Tampa    Bay   \n",
       "ner_tags          1      2   0        0      3       0   0      3      4   \n",
       "langs            en     en  en       en     en      en  en     en     en   \n",
       "ner_tags_str  B-PER  I-PER   O        O  B-ORG       O   O  B-ORG  I-ORG   \n",
       "\n",
       "                 9   10  \n",
       "tokens         Rays   )  \n",
       "ner_tags          4   0  \n",
       "langs            en  en  \n",
       "ner_tags_str  I-ORG   O  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(panx_ch[\"en\"][\"train\"][122]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ko 안의 ner 갯 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [4097, 4495, 5845],\n",
       " 'validation': [2039, 2297, 2976],\n",
       " 'test': [2116, 2165, 2971]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_count = {}\n",
    "for k in [\"train\", \"validation\", \"test\"]:\n",
    "    all_content =[]\n",
    "    for i in panx_ch['ko'][k][\"ner_tags_str\"]:\n",
    "        for j in i:\n",
    "            if \"B\" in j:\n",
    "                all_content.append(j.split(\"-\")[-1])\n",
    "        per , org, loc =(all_content.count('PER'),\n",
    "                         all_content.count('ORG'),\n",
    "                         all_content.count('LOC'))\n",
    "        all_count[k] = [per, org, loc]\n",
    "all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PER</th>\n",
       "      <th>ORG</th>\n",
       "      <th>LOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>4097</td>\n",
       "      <td>4495</td>\n",
       "      <td>5845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>2039</td>\n",
       "      <td>2297</td>\n",
       "      <td>2976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>2116</td>\n",
       "      <td>2165</td>\n",
       "      <td>2971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PER   ORG   LOC\n",
       "train       4097  4495  5845\n",
       "validation  2039  2297  2976\n",
       "test        2116  2165  2971"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_count,\n",
    "              index=['PER', 'ORG', 'LOC']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'넘겨주기 무명 용사의 무덤'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(panx_ch['ko']['train'][1266][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 11873, 2293, 6, 166637, 71393, 16907, 12286, 80169, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_tokenizer(' '.join(panx_ch['ko']['train'][4][\"tokens\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 17:04:33.326184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732521873.371438   68184 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732521873.385261   68184 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 17:04:33.484245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels # 라벨 수\n",
    "        # 모델 바디 로드\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # 분류 헤드\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # 가중치 초기화\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                token_type_ids=None, **kwargs):\n",
    "        # 인코더 아웃풋\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask = attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # 아웃풋 -> 헤드\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # 모델 출력 반환\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels = len(labels),\n",
    "                                         id2label = i2l,\n",
    "                                         label2id = l2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁this</td>\n",
       "      <td>▁is</td>\n",
       "      <td>▁test</td>\n",
       "      <td>,</td>\n",
       "      <td>▁here</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1    2      3  4      5     6\n",
       "tokens  <s>  ▁this  ▁is  ▁test  ,  ▁here  </s>\n",
       "tags      O      O    O      O  O      O     O"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # 토크나이징\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    \n",
    "    # seq 2 input_ids\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # logits\n",
    "    outputs = model(input_ids)[0]\n",
    "\n",
    "    # 가장 확율높은 클래스 선택\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "    # to df\n",
    "    preds = [tags[p] for p in predictions[0]]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"tokens\",\"tags\"])\n",
    "\n",
    "tag_text(\"this is test, here\", labels, xlmr_model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_ch['ko']['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징, 어텐션 마스크, 라벨 정리\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_input = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                     is_split_into_words=True)\n",
    "\n",
    "    labels=[]\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # print(examples['tokens'][idx],label)\n",
    "        word_ids = tokenized_input.word_ids(batch_index=idx)\n",
    "        # print(word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # print(word_idx)\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_input[\"labels\"] = labels\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인풋값 정제하기\n",
    "def encode_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=[\"langs\", \"ner_tags\", \"tokens\", \"ner_tags_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8231d54db8845468c6b644b9be24c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8164b173cb41a6b5699d9ef7b03962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bc206a7b7a4c0ab7843909322407c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ko =encode_dataset(panx_ch[\"ko\"])\n",
    "encoded_ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-RoBERTa 파인튜닝 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/anaconda3/envs/transformer/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epoch = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(encoded_ko[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-ko\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epoch,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def align_preds(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    label_list, preds_list = [],[]\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [],[]\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100: # <s> 및 </s> 무시\n",
    "                example_labels.append(i2l[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(i2l[preds[batch_idx][seq_idx]])\n",
    "        label_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "    \n",
    "    return preds_list, label_list\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_preds(eval_pred.predictions,\n",
    "                                 eval_pred.label_ids)\n",
    "    return {\"f1_score\" : f1_score(y_pred,y_true)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 콜레이터(Data Collator)\n",
    "    - 데이터 콜레이터는 데이터셋 요소들의 리스트를 입력으로 사용하여 배치를 형성하는 객체.\n",
    "    - 이러한 요소들은 train_dataset 또는 eval_dataset의 요소들과 동일한 타입.\n",
    "    - 배치를 구성하기 위해 데이터 콜레이터는 패딩과 같은 처리를 적용할 수 있다.\n",
    "    - DataCollatorForLanguageModeling과 같은 일부 콜레이터는 형성된 배치에 무작위 마스킹과 같은 일부 무작위 데이터 증강을 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification # 입력과 레이블 패딩 전용 콜레이터\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_init 함수 : 모델 재사용을 위한 모델 생성 함수정의\n",
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68184/2119604388.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model_init=model_init,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1251' max='1251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1251/1251 08:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.271479</td>\n",
       "      <td>0.821335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.244721</td>\n",
       "      <td>0.847851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.226946</td>\n",
       "      <td>0.864984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1251, training_loss=0.2776064072533858, metrics={'train_runtime': 501.7434, 'train_samples_per_second': 59.792, 'train_steps_per_second': 2.493, 'total_flos': 835350415434480.0, 'train_loss': 0.2776064072533858, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 객체를 trainer 에 전달\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=encoded_ko[\"train\"],\n",
    "                  eval_dataset=encoded_ko[\"validation\"],\n",
    "                  tokenizer = xlmr_tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(commit_message=\"first training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁안녕하세요</td>\n",
       "      <td>▁저는</td>\n",
       "      <td>▁서울</td>\n",
       "      <td>에</td>\n",
       "      <td>▁살고</td>\n",
       "      <td>▁있습니다</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1    2      3      4    5      6  7  8     9\n",
       "tokens  <s>  ▁안녕하세요  ▁저는    ▁서울      에  ▁살고  ▁있습니다  .  .  </s>\n",
       "tags      O       O    O  B-LOC  B-LOC    O      O  O  O     O"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트\n",
    "test_massage = \"안녕하세요 저는 서울에 살고 있습니다..\"\n",
    "tag_text(test_massage,\n",
    "         labels, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "# 전체 데이터셋에 대한 분석\n",
    "def forward_pass_with_label(batch):\n",
    "    # batch to dict\n",
    "    features = [dict(zip(batch,t)) for t in zip(*batch.values())]\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    # 추론 모드\n",
    "    with torch.no_grad():\n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        predicted_label = torch.argmax(output.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # 배치마다 손실 계산\n",
    "    # 하나의 토큰마다 7개의 확률이 나오므로 각 출력마다 하나의 텐서로\n",
    "    # 즉 정답 라벨 1개, 출력값 7개 에 대한 cross entropy loss 값이 나온다.\n",
    "    loss = cross_entropy(output.logits.view(-1,7),\n",
    "                         labels.view(-1), reduction=\"none\")\n",
    "\n",
    "    # loss 계산후 각 시퀀스별 묶기\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e423c4dd92c4e7c9423fd8b4e3bfcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증 데이터 배치별 매핑\n",
    "valid_result = encoded_ko['validation'].map(forward_pass_with_label, batch_size=32, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2l[-100] = \"IGN\" # 할당 <s>,</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(valid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 index to token\n",
    "df_result[\"input_tokens\"] = df_result[\"input_ids\"].apply(\n",
    "    lambda x : xlmr_tokenizer.convert_ids_to_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"predicted_label\"] = df_result[\"predicted_label\"].apply(\n",
    "    lambda x : [i2l[i] for i in x])\n",
    "\n",
    "df_result[\"labels\"] = df_result[\"labels\"].apply(\n",
    "    lambda x : [i2l[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 크기에 맞춰서 패딩 자르기\n",
    "df_result['loss'] = df_result.apply(\n",
    "    lambda x : x['loss'][:len(x['input_ids'])], axis=1)\n",
    "\n",
    "df_result[\"predicted_label\"] = df_result.apply(\n",
    "    lambda x : x[\"predicted_label\"][:len(x['input_ids'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 6339, 16295, 2099, 7286, 3999, 769, 96897,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, O, O, O, B-LOC, B-LOC, I-LOC, I...</td>\n",
       "      <td>[0.0, 0.0008671099785715342, 0.001348301419056...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, B-LOC, I-LOC, I-L...</td>\n",
       "      <td>[&lt;s&gt;, ▁지, 청, 사, 무, 소, 는, ▁아마, 미, ▁, 시에, ▁위치한, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 61805, 23854, 167427, 11031, 7286, 6521, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, B-ORG, B-ORG, I-ORG, I-ORG, I-O...</td>\n",
       "      <td>[0.0, 0.00034791138023138046, 0.00037222131504...</td>\n",
       "      <td>[O, O, O, O, B-ORG, B-ORG, I-ORG, I-ORG, I-ORG...</td>\n",
       "      <td>[&lt;s&gt;, ▁넘, 겨, 주기, ▁상, 무, ▁신, 협, ▁배, 구, 단, &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 7063, 8333, 123870, 15, 106, 6192, 6780, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-PER, B-PER, B-PER, O, O, O, O, O, O, O...</td>\n",
       "      <td>[0.0, 0.006986474618315697, 0.0048017664812505...</td>\n",
       "      <td>[O, B-PER, B-PER, B-PER, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁정, 형, 돈, ▁(, ▁1, 회, ▁~, ▁24, 회, ▁), &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 79978, 96648, 28185, 63993, 697, 97670, 16...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, O, O, O, O, O, O, O, B-ORG, B-ORG,...</td>\n",
       "      <td>[0.0, 0.0003415954706724733, 0.000439785566413...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-ORG, B-ORG, B...</td>\n",
       "      <td>[&lt;s&gt;, ▁그의, ▁시즌, ▁첫, ▁골, 은, ▁3-0, 으로, ▁이, 긴, ▁,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 242, 5106, 23526, 12057, 5106, 242, 769, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, B-LOC, B-LOC, O, O, O, B-LOC, B-LO...</td>\n",
       "      <td>[0.0, 0.00037126801908016205, 0.00037567710387...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-LOC, B-LOC, B-LOC, ...</td>\n",
       "      <td>[&lt;s&gt;, ▁', ▁'', ▁우, 파, ▁'', ▁', 는, ▁러시아, ▁바, 시키...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>[0, 101278, 6, 106286, 367, 6, 237558, 12788, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, ...</td>\n",
       "      <td>[0.0, 0.4214648902416229, 0.3684261739253998, ...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O,...</td>\n",
       "      <td>[&lt;s&gt;, ▁유럽, ▁, 연합, 의, ▁, 깃, 발, 에는, ▁12, 개의, ▁별,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>[0, 8237, 9187, 46364, 15, 33892, 84386, 713, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-PER, B-PER, B-PER, I-PER, I-PER, I-PER...</td>\n",
       "      <td>[0.0, 0.008504010736942291, 0.0070904060266911...</td>\n",
       "      <td>[O, B-PER, B-PER, B-PER, I-PER, I-PER, I-PER, ...</td>\n",
       "      <td>[&lt;s&gt;, ▁김, 학, 순, ▁(, ▁여성, 운동, 가, ▁), &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>[0, 44486, 6308, 3497, 8495, 1048, 5807, 10693...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-PER, B-PER, B-PER, B-PER, B-PER, I-PER...</td>\n",
       "      <td>[0.0, 0.005169240292161703, 0.0046163178049027...</td>\n",
       "      <td>[O, B-PER, B-PER, B-PER, B-PER, B-PER, I-PER, ...</td>\n",
       "      <td>[&lt;s&gt;, ▁레, 오, 나, 르, 도, ▁보, 누, 치, ▁(, ▁34, ▁), &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>[0, 13745, 26141, 11289, 45504, 26191, 45981, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[0.0, 0.003446117974817753, 0.0061222868971526...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[&lt;s&gt;, ▁발, 효, 하지, ▁않고, ▁화, 덕, 에, ▁구, 운, ▁, 납, 작...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[0, 1504, 1083, 115676, 1504, 26141, 40771, 16...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, B-PER, B-PER, B-PER, O, O, O, O...</td>\n",
       "      <td>[0.0, 0.000390215078368783, 0.0004285847535356...</td>\n",
       "      <td>[O, O, O, O, B-PER, B-PER, B-PER, O, O, O, O, ...</td>\n",
       "      <td>[&lt;s&gt;, ▁이, 로, ▁인해, ▁이, 효, 리는, ▁활동을, ▁임, 시, ▁중단,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_ids  \\\n",
       "0     [0, 6339, 16295, 2099, 7286, 3999, 769, 96897,...   \n",
       "1     [0, 61805, 23854, 167427, 11031, 7286, 6521, 5...   \n",
       "2     [0, 7063, 8333, 123870, 15, 106, 6192, 6780, 7...   \n",
       "3     [0, 79978, 96648, 28185, 63993, 697, 97670, 16...   \n",
       "4     [0, 242, 5106, 23526, 12057, 5106, 242, 769, 1...   \n",
       "...                                                 ...   \n",
       "4995  [0, 101278, 6, 106286, 367, 6, 237558, 12788, ...   \n",
       "4996  [0, 8237, 9187, 46364, 15, 33892, 84386, 713, ...   \n",
       "4997  [0, 44486, 6308, 3497, 8495, 1048, 5807, 10693...   \n",
       "4998  [0, 13745, 26141, 11289, 45504, 26191, 45981, ...   \n",
       "4999  [0, 1504, 1083, 115676, 1504, 26141, 40771, 16...   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "4995  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4996                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4997            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4998  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4999      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                                 labels  \\\n",
       "0     [IGN, O, O, O, O, O, O, B-LOC, B-LOC, I-LOC, I...   \n",
       "1     [IGN, O, O, O, B-ORG, B-ORG, I-ORG, I-ORG, I-O...   \n",
       "2     [IGN, B-PER, B-PER, B-PER, O, O, O, O, O, O, O...   \n",
       "3     [IGN, O, O, O, O, O, O, O, O, O, B-ORG, B-ORG,...   \n",
       "4     [IGN, O, O, B-LOC, B-LOC, O, O, O, B-LOC, B-LO...   \n",
       "...                                                 ...   \n",
       "4995  [IGN, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, ...   \n",
       "4996  [IGN, B-PER, B-PER, B-PER, I-PER, I-PER, I-PER...   \n",
       "4997  [IGN, B-PER, B-PER, B-PER, B-PER, B-PER, I-PER...   \n",
       "4998  [IGN, O, O, O, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "4999  [IGN, O, O, O, B-PER, B-PER, B-PER, O, O, O, O...   \n",
       "\n",
       "                                                   loss  \\\n",
       "0     [0.0, 0.0008671099785715342, 0.001348301419056...   \n",
       "1     [0.0, 0.00034791138023138046, 0.00037222131504...   \n",
       "2     [0.0, 0.006986474618315697, 0.0048017664812505...   \n",
       "3     [0.0, 0.0003415954706724733, 0.000439785566413...   \n",
       "4     [0.0, 0.00037126801908016205, 0.00037567710387...   \n",
       "...                                                 ...   \n",
       "4995  [0.0, 0.4214648902416229, 0.3684261739253998, ...   \n",
       "4996  [0.0, 0.008504010736942291, 0.0070904060266911...   \n",
       "4997  [0.0, 0.005169240292161703, 0.0046163178049027...   \n",
       "4998  [0.0, 0.003446117974817753, 0.0061222868971526...   \n",
       "4999  [0.0, 0.000390215078368783, 0.0004285847535356...   \n",
       "\n",
       "                                        predicted_label  \\\n",
       "0     [O, O, O, O, O, O, O, B-LOC, B-LOC, I-LOC, I-L...   \n",
       "1     [O, O, O, O, B-ORG, B-ORG, I-ORG, I-ORG, I-ORG...   \n",
       "2      [O, B-PER, B-PER, B-PER, O, O, O, O, O, O, O, O]   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, B-ORG, B-ORG, B...   \n",
       "4     [O, O, O, O, O, O, O, O, B-LOC, B-LOC, B-LOC, ...   \n",
       "...                                                 ...   \n",
       "4995  [O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O, O,...   \n",
       "4996  [O, B-PER, B-PER, B-PER, I-PER, I-PER, I-PER, ...   \n",
       "4997  [O, B-PER, B-PER, B-PER, B-PER, B-PER, I-PER, ...   \n",
       "4998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4999  [O, O, O, O, B-PER, B-PER, B-PER, O, O, O, O, ...   \n",
       "\n",
       "                                           input_tokens  \n",
       "0     [<s>, ▁지, 청, 사, 무, 소, 는, ▁아마, 미, ▁, 시에, ▁위치한, ...  \n",
       "1        [<s>, ▁넘, 겨, 주기, ▁상, 무, ▁신, 협, ▁배, 구, 단, </s>]  \n",
       "2      [<s>, ▁정, 형, 돈, ▁(, ▁1, 회, ▁~, ▁24, 회, ▁), </s>]  \n",
       "3     [<s>, ▁그의, ▁시즌, ▁첫, ▁골, 은, ▁3-0, 으로, ▁이, 긴, ▁,...  \n",
       "4     [<s>, ▁', ▁'', ▁우, 파, ▁'', ▁', 는, ▁러시아, ▁바, 시키...  \n",
       "...                                                 ...  \n",
       "4995  [<s>, ▁유럽, ▁, 연합, 의, ▁, 깃, 발, 에는, ▁12, 개의, ▁별,...  \n",
       "4996          [<s>, ▁김, 학, 순, ▁(, ▁여성, 운동, 가, ▁), </s>]  \n",
       "4997  [<s>, ▁레, 오, 나, 르, 도, ▁보, 누, 치, ▁(, ▁34, ▁), <...  \n",
       "4998  [<s>, ▁발, 효, 하지, ▁않고, ▁화, 덕, 에, ▁구, 운, ▁, 납, 작...  \n",
       "4999  [<s>, ▁이, 로, ▁인해, ▁이, 효, 리는, ▁활동을, ▁임, 시, ▁중단,...  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6339</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>O</td>\n",
       "      <td>▁지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16295</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>O</td>\n",
       "      <td>청</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2099</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>O</td>\n",
       "      <td>사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7286</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>O</td>\n",
       "      <td>무</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3999</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>O</td>\n",
       "      <td>소</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2166</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>O</td>\n",
       "      <td>시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>206276</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>O</td>\n",
       "      <td>▁중단</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>50751</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>O</td>\n",
       "      <td>하였다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>O</td>\n",
       "      <td>▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89119 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     input_ids attention_mask labels      loss predicted_label input_tokens\n",
       "0         6339              1      O  0.000867               O           ▁지\n",
       "0        16295              1      O  0.001348               O            청\n",
       "0         2099              1      O  0.001185               O            사\n",
       "0         7286              1      O  0.001066               O            무\n",
       "0         3999              1      O  0.001376               O            소\n",
       "...        ...            ...    ...       ...             ...          ...\n",
       "4999      2166              1      O  0.000398               O            시\n",
       "4999    206276              1      O  0.000371               O          ▁중단\n",
       "4999     50751              1      O  0.000279               O          하였다\n",
       "4999         6              1      O  0.000228               O            ▁\n",
       "4999         5              1      O  0.000276               O            .\n",
       "\n",
       "[89119 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 각 토큰별 분해\n",
    "df_tokens = df_result.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens[df_tokens[\"labels\"] != \"IGN\"] # IGN은 예측 loss 에서 빼기\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6505</td>\n",
       "      <td>53513</td>\n",
       "      <td>6374</td>\n",
       "      <td>6025</td>\n",
       "      <td>9839</td>\n",
       "      <td>4596</td>\n",
       "      <td>2267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.766222</td>\n",
       "      <td>0.085303</td>\n",
       "      <td>0.482868</td>\n",
       "      <td>0.454054</td>\n",
       "      <td>0.20701</td>\n",
       "      <td>0.35287</td>\n",
       "      <td>0.520431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>4984.272879</td>\n",
       "      <td>4564.80906</td>\n",
       "      <td>3077.798421</td>\n",
       "      <td>2735.678038</td>\n",
       "      <td>2036.774456</td>\n",
       "      <td>1621.78849</td>\n",
       "      <td>1179.816505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0           1            2            3            4  \\\n",
       "labels        B-ORG           O        I-ORG        B-PER        B-LOC   \n",
       "count          6505       53513         6374         6025         9839   \n",
       "mean       0.766222    0.085303     0.482868     0.454054      0.20701   \n",
       "sum     4984.272879  4564.80906  3077.798421  2735.678038  2036.774456   \n",
       "\n",
       "                 5            6  \n",
       "labels       I-PER        I-LOC  \n",
       "count         4596         2267  \n",
       "mean       0.35287     0.520431  \n",
       "sum     1621.78849  1179.816505  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]]\n",
    "    .agg([\"count\",\"mean\",\"sum\"])\n",
    "    .droplevel(level=0, axis=1) # 멀티 컬럼 삭제\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- B-ORG 의 평균 손실이 가장 높다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제로샷 전이 : 교차 언어 전이학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a1fbe95b564ad191d3e05cd4cd06d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027de59a45a74f03aacc351b14f781b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afef92fbf6b04bdda7c27ee6b9b4de28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_en = encode_dataset(panx_ch[\"en\"])\n",
    "encoded_ja = encode_dataset(panx_ch[\"ja\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5389431167956916"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_f1_score(trainer ,encoded_en[\"valiation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20663811563169163"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_f1_score(trainer ,encoded_ja[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 06:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>1.082186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>1.007886</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.922073</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1e7433272e49a9846c34627bd0c31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1732539057.DESKTOP-2KLMMI4.68184.3:   0%|          | 0.00/7.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sample</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sample  f1_score\n",
       "0         250       0.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset[\"validation\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "\n",
    "    training_args.logging_steps = len(train_ds) // batch_size\n",
    "    trainer = Trainer(model_init=model_init,\n",
    "                      args=training_args,\n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds,\n",
    "                      eval_dataset=valid_ds)\n",
    "    \n",
    "    trainer.train()\n",
    "    if training_args.push_to_hub:\n",
    "        trainer.push_to_hub(commit_message = \"more_train\")\n",
    "    \n",
    "    f1_score = get_f1_score(trainer, test_ds)   \n",
    "    \n",
    "    return pd.DataFrame.from_dict({\n",
    "        \"num_sample\" : [len(train_ds)], \"f1_score\" : [f1_score]})\n",
    "\n",
    "train_on_subset(encoded_ja, 250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
