- [00_Attention.ipynb](00_Attention.ipynb)
  - 1. RNN 구조의 인코더 디코더의 문제점
  - 2. 인코더 디코더에서의 어텐션 매커니즘
  - 3. ULMFiT(Universal Language Model Fine-tuning)
  - 4. 셀프 어텐션과 전이학습을 이용한 모델들
- [01_huggingface_간단구현.ipynb](01_huggingface_간단구현.ipynb)
  - 허깅페이스
  - transformer pipeline - 텍스트 분류
  - transformer pipeline - 개체명 인식(Ner)
  - transformer pipeline - 질의 응답
  - transformer pipeline - 요약
  - transformer pipeline - 번역
  - transformer pipeline - 텍스트 생성
- [02_text_classification.ipynb](02_text_classification.ipynb)
  - 진행 순서
  - 데이터셋
  - 데이터 셋 to Dataframe
  - **데이터 클래스 분포 확인**
  - 문장 길이 확인(최대 토큰 길이 측정)
  - 문장 토큰화 하기
  - 부분 단어 토큰화
  - 전체 데이터 셋 토큰화
  - DistilBERT 훈련하기
  - DistilBERT 예측 분류(machine learning head)
  - distilBERT(End to End finetuning)
  - cf) keras로 훈련 시키기(예시)
  - 직접 훈련한 모델 불러와서 사용 해보기(pipeline)
- [03_Transformer.ipynb](03_Transformer.ipynb)
  - **어텐션 가중치(attention weights)**
  - seq2seq 의 attention 매커니즘(인코더 디코더 어텐션)
  - 기존 seq2seq 및 + attention 이용의 단점
  - [트랜스 포머](https://arxiv.org/pdf/1706.03762)
  - 다양한 트랜스 포머 모델들
  - **셀프 어텐션**
  - 어텐션 시각화
  - 밀집 임베딩 만들기
  - 셀프 어텐션 구하기(임베딩 된 벡터들을 이용하여 유사도 계산)
  - bert-base-uncased의 어텐션 헤드 출력 확인
  - Position Wise Feed Forward Layer
  - 층 정규화 및 스킵 커넥션
  - 인코더 구현
  - 위치 임베딩 추가하기
  - 상대적 위치 표현을 포함한 트랜스 포머의 인코더
  - 분류 헤드 추가하기
  - 디코더 의 어텐션
  - 평가 방법, 및 데이터셋
  - 다양한 트랜스포머 모델들
- [04_NER(다중_언어_개체명_인식).ipynb](04_NER(다중_언어_개체명_인식).ipynb)
  - 다중언어 개체명인식
  - 언어 선택하기
  - 태그 분포 확인하기
  - 다중 언어 트랜스 포머
  - XLM-R(Cross-lingual LM)
  - 토큰화 파이프라인
  - 트랜스 포머 모델 클래스의 형식
  - xtreme 데이터 셋 -> 커널 종료 후 아래 코드 부터 다시 실행
  - 데이터 셋 로드 및 전처리
  - XLM-R 토크나이저
  - 모델 생성
  - XLM-RoBERTa 파인튜닝 하기
  - 제로샷 전이 : 교차 언어 전이학습
  - 다국어 동시 파인튜닝 진행하기
- [05_text_gen.ipynb](05_text_gen.ipynb)
  - 텍스트 생성
  - 그리디 서치 디코딩 (Greedy Search Decoding)
  - 그리디 서치 디코딩의 gpt-2 예시
  - generate() 함수
  - 빔 서치 디코딩(with $\log$ probability and `no_repeat_ngram_size`)
