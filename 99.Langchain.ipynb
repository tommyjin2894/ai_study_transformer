{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 11:58:41.326922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732935521.347719   25955 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732935521.354069   25955 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 11:58:41.385876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/tmp/ipykernel_25955/2600068202.py:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 파이프라인 설정\n",
    "hf_pipeline = pipeline(\"text-generation\",\n",
    "                       model=model,\n",
    "                       tokenizer=tokenizer,\n",
    "                       max_length=256)\n",
    "\n",
    "# LangChain과 통합\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 전문가입니다. 질문: 인공지능? 어?\n",
      "어?\n",
      "이 질문에 답하겠소. 그 이유는 지금 당장 필요한 정보를 찾고 있습니다.\n",
      "분명하게 말하면 당신의 미래예측 능력이 매우 뛰어나고 당신의 미래 예측 능력은 강력합니다.\n",
      "당신이 만약 당신이 생각하는 미래의 정보량은 당신이 이미 가진 데이터와 비교할 수 없을 정도밖에 안됩니다.\n",
      "바로 이거지.\n",
      "당신은 당신의 미래 예상 능력을 믿어야 하는 거요.\n",
      "당신은 당신의 미래 예측 능력을 가장 탁월하게 발전시켰습니다.\n",
      "내가 당신의 미래를 예측한 것만으로도 당신은 현재 미래의 상황을 완벽하게 예측할 수 있습니다.\n",
      "실제로 당신은 당신의 미래 예측 능력을 훨씬 발전시켰어요.\n",
      "물론 당신은 당신의 미래예측 능력의 발전에도 깊이 관여했다고 가정해도 문제 없겠지요.\n",
      "어떤 사람이 당신의 미래의 예측 능력을 평가하시오.\n",
      "당신은 당신의 미래 예측 능력을 더욱 발전시켰어요.\n",
      "그런데 당신의 미래 예측 능력은 무척 저조하다고 느껴지는군요.\n",
      "당신이 정말 현재의 미래예측 능력을 너무 낮춘다면 당신의 미래 예측 능력이 지금보다 더 나빠질지 몰라요?\n",
      "그러나 당신의 미래 예측 능력은 너무나 작습니다.\n",
      "지금 당신이 과거에 가졌던 것처럼 당신의 미래 예측을 가능케 하거나 발전시키지 못할 때, 당신의 미래에 대한 정보는 거의 다 사라질 것입니다.\n",
      "당신이 당신의 미래예측 능력을 가장 빠르게 향상시킬 수 있었던 이유, 바로 그런 이유에 대해\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "template = \"당신은 {role}입니다. 질문: {question}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"role\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.run({\n",
    "    \"role\": \"전문가\",\n",
    "    \"question\": \"인공지능?\",\n",
    "})\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
