{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 허깅페이스\n",
    "- [토크나이저](https://huggingface.co/docs/tokenizers/index)\n",
    "- [데이터셋](https://huggingface.co/docs/datasets/index)\n",
    "- [액셀러 레이트](https://huggingface.co/docs/accelerate/index)\n",
    "- 해결 해야 할 과제\n",
    "    - 언어의 다양성\n",
    "    - 데이터의 한계\n",
    "    - 긴 문서 처리\n",
    "    - 불 투명성(black box problem)\n",
    "    - 편향성(윤리적 문제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer pipeline - 텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classfier = pipeline(\"text-classification\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"Nvidia Corporation is a major American technology company, \\\n",
    "    best known for its graphics processing units (GPUs) that are widely used in gaming, professional graphics, \\\n",
    "    and high-performance computing. Founded in 1993 and headquartered in Santa Clara, \\\n",
    "    California, Nvidia has grown into a key player in various industries, including artificial intelligence (AI),\\\n",
    "     data science, automotive technology, and mobile computing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.99914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label    score\n",
       "0  POSITIVE  0.99914"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output = classfier(review_text)\n",
    "pd.DataFrame(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer pipeline - 개체명 인식(Ner)\n",
    "- ORG = 조직\n",
    "- MISC = 기타\n",
    "- LOC = 위치\n",
    "\n",
    "등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>score</th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>2</td>\n",
       "      <td>##vid</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>3</td>\n",
       "      <td>##ia</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.999056</td>\n",
       "      <td>4</td>\n",
       "      <td>Corporation</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>0.997677</td>\n",
       "      <td>8</td>\n",
       "      <td>American</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.998050</td>\n",
       "      <td>46</td>\n",
       "      <td>Santa</td>\n",
       "      <td>243</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.997077</td>\n",
       "      <td>47</td>\n",
       "      <td>Clara</td>\n",
       "      <td>249</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.997530</td>\n",
       "      <td>49</td>\n",
       "      <td>California</td>\n",
       "      <td>260</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>51</td>\n",
       "      <td>N</td>\n",
       "      <td>272</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>52</td>\n",
       "      <td>##vid</td>\n",
       "      <td>273</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>53</td>\n",
       "      <td>##ia</td>\n",
       "      <td>276</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity     score  index         word  start  end\n",
       "0    I-ORG  0.999609      1            N      0    1\n",
       "1    I-ORG  0.999246      2        ##vid      1    4\n",
       "2    I-ORG  0.999594      3         ##ia      4    6\n",
       "3    I-ORG  0.999056      4  Corporation      7   18\n",
       "4   I-MISC  0.997677      8     American     30   38\n",
       "5    I-LOC  0.998050     46        Santa    243  248\n",
       "6    I-LOC  0.997077     47        Clara    249  254\n",
       "7    I-LOC  0.997530     49   California    260  270\n",
       "8    I-ORG  0.998946     51            N    272  273\n",
       "9    I-ORG  0.999167     52        ##vid    273  276\n",
       "10   I-ORG  0.999258     53         ##ia    276  278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = ner_tagger(review_text)\n",
    "pd.DataFrame(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer pipeline - 질의 응답\n",
    "- 추출적 질문 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "answer = pipeline(\"question-answering\", device=\"cuda\")\n",
    "question = \"what is gpu?\"\n",
    "output = answer(question = question, context = review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphics processing units'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer pipeline - 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your min_length=56 must be inferior than your max_length=12.\n",
      "/home/tommy/anaconda3/envs/transformer/lib/python3.11/site-packages/transformers/generation/utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (12). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summrizer = pipeline(\"summarization\", device=\"cuda\")\n",
    "output = summrizer(review_text,max_length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nvidia Corporation is a major American technology company .\n"
     ]
    }
   ],
   "source": [
    "print(output[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer pipeline - 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "translater = pipeline(\"translation_en_to_de\",\n",
    "                      model=\"Helsinki-NLP/opus-mt-en-de\",\n",
    "                      device=\"cuda\")\n",
    "output = translater(review_text, clean_up_tokenization_spaces=True, min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nvidia Corporation ist ein großes amerikanisches Technologieunternehmen, das am besten für seine Grafikverarbeitungseinheiten (GPUs) bekannt ist, die weit verbreitet in Gaming, professioneller Grafik und Hochleistungs-Computing verwendet werden. Nvidia wurde 1993 gegründet und hat seinen Hauptsitz in Santa Clara, Kalifornien, und ist zu einem wichtigen Akteur in verschiedenen Branchen gewachsen, einschließlich künstlicher Intelligenz (KI), Datenwissenschaft, Automobiltechnologie und mobiles Computing..................................................................................................................................................................................................................'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer pipeline - 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline(\"text-generation\")\n",
    "input_text = \"me\" + review_text + \"\\nanswer:\"\n",
    "output = text_gen(input_text, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "answer: There is no such thing as the \"Nvidia GPUs\" listed above. In fact the Nvidia GPUs are sold as part of the Intel \"Nvidia Series D\" which consist of the \"Nvidia GPUs 3xxx, Kepler, and Kepler Mega APUs --------------\" series and \"Nvidia GPUs 3xxx, Kepler Mega APUs 3\", or \"Nvidia GeForce GPUs 3xxx, --------------\" series. The    GPUs are known as \"Nvidia GPUs Pro\"  (GPUs 3xx\n"
     ]
    }
   ],
   "source": [
    "print(output[0][\"generated_text\"].split(review_text)[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
