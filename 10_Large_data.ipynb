{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대규모 데이터셋\n",
    "- 사용해야 될 경우\n",
    "    - 가용 훈련 데이터의 수가 pretrained 모델 훈련시의 데이터 양과 비슷할 때\n",
    "    - 도메인의 차이 가 클때\n",
    "- 조심해야 할 점\n",
    "    - (반)자동으로 생성된 데이터가 많기 때문에 데이터 품질이 낮을 수 있다.\n",
    "    - 편향성, 낮은 품질, **저작권 위반** 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 같은 모델, 다른 데이터셋 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "gpt_pipe_1 = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "gpt_pipe_2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파라미터 수 비교\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225310, 237137)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"파라미터 수 비교\")\n",
    "(sum([len(p) for p in gpt_pipe_1.model.parameters()]), \n",
    " sum([len(p) for p in gpt_pipe_2.model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\n UDA is no - kill. \" \\n \" he\\'s a little young to be a ranger to a ranger. \" \\n \" no - kill ain\\'t no cowboy. i\\'m only a tracker for the local rangers. \" \\n \" no - kill'},\n",
       " {'generated_text': '\\n UDA is the very first one in the line who can speak. so far, it\\'s been an exceptionally hard night. \" \\n \" does the wolf really belong to you? \" \\n \" he does. \" \\n i stared at him, perplexed'},\n",
       " {'generated_text': \"\\n UDA isn't dead. her life is still in the water. \\n he wanted to believe her. needed to believe it. \\n but she 'd gotten him into this. there was no other way to prove her story, no other way to prove\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\n UDA is an open source game engine for Linux that brings together the best technologies of the desktop. In a recent blog post, UDA developer John Daley explains that in many respects this open source effort was the perfect vehicle for a \"modern'},\n",
       " {'generated_text': '\\n UDA is the most expensive driver to purchase in India, where around $450 million is spent in 2013-14 (see chart ). The value of the RTE licence can be measured in USD, which gives the price. For RTE buyers'},\n",
       " {'generated_text': \"\\n UDA is on the horizon.\\n\\nAnd maybe most damning, this is just the latest in a long line of examples of UDA's failure to meet its commitments to protect the U.S. from China's cyber attacks.\\n\\n\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 같은 입력에 대한 출력 비교\n",
    "def pipe_out(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "        truncation=True,\n",
    "        )\n",
    "    return out\n",
    "\n",
    "display(pipe_out(gpt_pipe_1, '\\n UDA is', 3)) # 로멘스 소설 데이터\n",
    "print(\"*\"*50)\n",
    "display(pipe_out(gpt_pipe_2, '\\n UDA is', 3)) # 레딧 기사 텍스트 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gpt_pipe_1\n",
    "del gpt_pipe_2\n",
    "\n",
    "import torch\n",
    "\n",
    "# CUDA 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 메모리 사용량 확인\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code dataset 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git: 'file-000000000000.json.gz' is not a git command. See 'git --help'.\n"
     ]
    }
   ],
   "source": [
    "!cd codeparrot/ && git file-000000000000.json.gz pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'codeparrot' already exists and is not an empty directory.\n",
      "Downloading LFS objects: 100% (184/184), 46 GB | 58 MB/s                        \r"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/transformersbook/codeparrot\n",
    "!cd codeparrot && git lfs pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대용량 데이터셋 다루기\n",
    "- 약 46gb 의 code 데이터셋 -> 압축 해제시 200gb 정도\n",
    "- 메모리 매핑과 스트리밍 기능 이용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 매핑\n",
    "- 제로 카피 + 제로-오버헤드 메모리 매핑\n",
    "- 파일로 디스크에 캐싱됨\n",
    "- 데이터셋을 로딩되는 대신, 포인터를 열어 대신 사용(필요할때마다 캐싱된 파일로 불러온다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 184/184 [00:00<00:00, 1527.57files/s]\n",
      "Generating train split: 18695559 examples [59:12, 5262.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "dataset = load_dataset(\"./codeparrot\", split=\"train\",\n",
    "                       download_config=download_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 갯수 : 18695559\n",
      "캐시 : 183.59GB\n",
      "메모리 사용량 : 1463 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "print(f\"데이터셋 갯수 : {len(dataset)}\")\n",
    "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "print(f\"캐시 : {ds_size/ 2 ** 30:.2f}GB\")\n",
    "print(f\"메모리 사용량 : {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'ahmedbodi/AutobahnPython',\n",
       " 'path': 'examples/asyncio/websocket/echo/client_coroutines.py',\n",
       " 'copies': '13',\n",
       " 'size': '2044',\n",
       " 'content': '###############################################################################\\n##\\n##  Copyright (C) 2013-2014 Tavendo GmbH\\n##\\n##  Licensed under the Apache License, Version 2.0 (the \"License\");\\n##  you may not use this file except in compliance with the License.\\n##  You may obtain a copy of the License at\\n##\\n##      http://www.apache.org/licenses/LICENSE-2.0\\n##\\n##  Unless required by applicable law or agreed to in writing, software\\n##  distributed under the License is distributed on an \"AS IS\" BASIS,\\n##  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n##  See the License for the specific language governing permissions and\\n##  limitations under the License.\\n##\\n###############################################################################\\n\\nfrom autobahn.asyncio.websocket import WebSocketClientProtocol, \\\\\\n                                       WebSocketClientFactory\\n\\nimport asyncio\\n\\n\\n\\nclass MyClientProtocol(WebSocketClientProtocol):\\n\\n   def onConnect(self, response):\\n      print(\"Server connected: {0}\".format(response.peer))\\n\\n   @asyncio.coroutine\\n   def onOpen(self):\\n      print(\"WebSocket connection open.\")\\n\\n      ## start sending messages every second ..\\n      while True:\\n         self.sendMessage(u\"Hello, world!\".encode(\\'utf8\\'))\\n         self.sendMessage(b\"\\\\x00\\\\x01\\\\x03\\\\x04\", isBinary = True)\\n         yield from asyncio.sleep(1)\\n\\n   def onMessage(self, payload, isBinary):\\n      if isBinary:\\n         print(\"Binary message received: {0} bytes\".format(len(payload)))\\n      else:\\n         print(\"Text message received: {0}\".format(payload.decode(\\'utf8\\')))\\n\\n   def onClose(self, wasClean, code, reason):\\n      print(\"WebSocket connection closed: {0}\".format(reason))\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n   import asyncio\\n\\n   factory = WebSocketClientFactory(\"ws://localhost:9000\", debug = False)\\n   factory.protocol = MyClientProtocol\\n\\n   loop = asyncio.get_event_loop()\\n   coro = loop.create_connection(factory, \\'127.0.0.1\\', 9000)\\n   loop.run_until_complete(coro)\\n   loop.run_forever()\\n   loop.close()\\n',\n",
       " 'license': 'apache-2.0'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 접근법 streamed_dataset[숫자] 같은 방식은 접근할 수 없다.\n",
    "\n",
    "streamed_dataset = load_dataset('./codeparrot', split='train', streaming=True) # IterableDataset\n",
    "next(iter(streamed_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원격 데이터 셋 (스트리밍으로 가져온다.)\n",
    "remote_dataset = load_dataset('transformersbook/codeparrot', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'ahmedbodi/AutobahnPython',\n",
       " 'path': 'examples/asyncio/websocket/echo/client_coroutines.py',\n",
       " 'copies': '13',\n",
       " 'size': '2044',\n",
       " 'content': '###############################################################################\\n##\\n##  Copyright (C) 2013-2014 Tavendo GmbH\\n##\\n##  Licensed under the Apache License, Version 2.0 (the \"License\");\\n##  you may not use this file except in compliance with the License.\\n##  You may obtain a copy of the License at\\n##\\n##      http://www.apache.org/licenses/LICENSE-2.0\\n##\\n##  Unless required by applicable law or agreed to in writing, software\\n##  distributed under the License is distributed on an \"AS IS\" BASIS,\\n##  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n##  See the License for the specific language governing permissions and\\n##  limitations under the License.\\n##\\n###############################################################################\\n\\nfrom autobahn.asyncio.websocket import WebSocketClientProtocol, \\\\\\n                                       WebSocketClientFactory\\n\\nimport asyncio\\n\\n\\n\\nclass MyClientProtocol(WebSocketClientProtocol):\\n\\n   def onConnect(self, response):\\n      print(\"Server connected: {0}\".format(response.peer))\\n\\n   @asyncio.coroutine\\n   def onOpen(self):\\n      print(\"WebSocket connection open.\")\\n\\n      ## start sending messages every second ..\\n      while True:\\n         self.sendMessage(u\"Hello, world!\".encode(\\'utf8\\'))\\n         self.sendMessage(b\"\\\\x00\\\\x01\\\\x03\\\\x04\", isBinary = True)\\n         yield from asyncio.sleep(1)\\n\\n   def onMessage(self, payload, isBinary):\\n      if isBinary:\\n         print(\"Binary message received: {0} bytes\".format(len(payload)))\\n      else:\\n         print(\"Text message received: {0}\".format(payload.decode(\\'utf8\\')))\\n\\n   def onClose(self, wasClean, code, reason):\\n      print(\"WebSocket connection closed: {0}\".format(reason))\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n   import asyncio\\n\\n   factory = WebSocketClientFactory(\"ws://localhost:9000\", debug = False)\\n   factory.protocol = MyClientProtocol\\n\\n   loop = asyncio.get_event_loop()\\n   coro = loop.create_connection(factory, \\'127.0.0.1\\', 9000)\\n   loop.run_until_complete(coro)\\n   loop.run_forever()\\n   loop.close()\\n',\n",
       " 'license': 'apache-2.0'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in remote_dataset:\n",
    "    display(data)  # 한 개의 데이터 샘플 출력\n",
    "    break  # 예시로 첫 번째 샘플만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "with open(\"hf_key_token.json\") as f:\n",
    "    token = json.load(f)[\"hf_key_token\"]\n",
    "\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 만들고, 허깅페이스에 커밋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- huggingface 저장소 생성 및 git clone\n",
    "- in bash\n",
    "    - 허깅페이스 로그인 깃 생성\n",
    "        ```bash\n",
    "        $ huggingface-cli login\n",
    "        $ huggingface-cli repo create --type dataset codeparrot-train\n",
    "        $ huggingface-cli repo create --type dataset codeparrot-valid\n",
    "        ```\n",
    "\n",
    "    - 가져오기\n",
    "        ```bash\n",
    "        $ git clone https://huggingface.co/datasets/tommyjin/codeparrot-valid\n",
    "        $ git clone https://huggingface.co/datasets/tommyjin/codeparrot-train\n",
    "        ```\n",
    "\n",
    "- 훈련세트로 복사\n",
    "    ```bash\n",
    "        $ cd codeparrot-train\n",
    "        $ cp ../codeparrot/*.json.gz .\n",
    "        $ rm ./file-000000000183.json.gz\n",
    "        $ git add .\n",
    "        $ git commit -m \"Adding dataset files\"\n",
    "        $ git push\n",
    "    ```\n",
    "- 검증 세트 복사 및 커밋\n",
    "    ```bash\n",
    "        $ cd ../codeparrot-valid\n",
    "        $ cp ../codeparrot/file-000000000183.json.gz .\n",
    "        $ mv ./file-000000000183.json.gz ./file-000000000183_validation.json.gz\n",
    "        $ git add .\n",
    "        $ git commit -m \"Adding dataset files\"\n",
    "        $ git push\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체적인 과정 :\n",
    "    1. 정규화\n",
    "    2. 사전 토큰화\n",
    "    3. 토크나이저 모델\n",
    "    4. 사후 처리\n",
    "\n",
    "- 다양한 알고리즘\n",
    "    - Byte Pair Encoding : 단일문자의 리스트로 시작해 점진적으로 새토큰 만들기(정해진 크기까지생성)\n",
    "    - Unigram : 모든 토큰을 만든후 점진적을 토큰 삭제 (정해진 크기까지)\n",
    "\n",
    "- 토크나이즈 성능 측정법\n",
    "    - 부분 단어 생산력(subword fertilty) : 토큰화된 단어마다 생성되는 부분단어의 평균갯수\n",
    "    - 연속 단어 비률(proportion of continued words) : 두개의 부분토큰으로 분할된 토큰화된 단어의 비율\n",
    "    - 커버리지 측정값(coverage metrics) : 알수없는 단어나, 거의 사용되지 않는 토큰의 비율\n",
    "    - **토크나이저 를 이용한 모델의 성능지표가 가장 중요하다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 토크나이저 확인하기\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "bytes_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = {v:k for k, v in bytes_to_unicode_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>\"</td>\n",
       "      <td>#</td>\n",
       "      <td>$</td>\n",
       "      <td>%</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>'</td>\n",
       "      <td>(</td>\n",
       "      <td>)</td>\n",
       "      <td>*</td>\n",
       "      <td>...</td>\n",
       "      <td>ĺ</td>\n",
       "      <td>Ļ</td>\n",
       "      <td>ļ</td>\n",
       "      <td>Ľ</td>\n",
       "      <td>ľ</td>\n",
       "      <td>Ŀ</td>\n",
       "      <td>ŀ</td>\n",
       "      <td>Ł</td>\n",
       "      <td>ł</td>\n",
       "      <td>Ń</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>155</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>158</td>\n",
       "      <td>159</td>\n",
       "      <td>160</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1   2   3   4   5   6   7   8   9    ...  246  247  248  249  250  251  \\\n",
       "0   !   \"   #   $   %   &   '   (   )   *  ...    ĺ    Ļ    ļ    Ľ    ľ    Ŀ   \n",
       "1  33  34  35  36  37  38  39  40  41  42  ...  152  153  154  155  156  157   \n",
       "\n",
       "   252  253  254  255  \n",
       "0    ŀ    Ł    ł    Ń  \n",
       "1  158  159  160  173  \n",
       "\n",
       "[2 rows x 256 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_dict(unicode_to_byte_map.items()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['def', 'ĠHello'], 50257)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt 토크나이저\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer(\"\"\"\\\n",
    "def HelloWorld():\n",
    "    print(\"hello)\n",
    "\"\"\").tokens()[:2], len(tokenizer) # 어휘사전 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BPE 토크나이저 <u>**통계값 추출 훈련**</u>\n",
    "    1. 목표 어휘사전 크기를 정하기\n",
    "    2. iterator 준비\n",
    "    3. train_new_iterator() 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 토큰 :\n",
      "[('ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ', 35496)]\n"
     ]
    }
   ],
   "source": [
    "print(f'''가장 긴 토큰 :\n",
    "{sorted(tokenizer.vocab.items(), key=lambda x:len(x[0]), reverse=True)[:1]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b953ce34caa4deab8cc8a5d0bd88631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "length = 100000\n",
    "dataset_name = 'tommyjin/codeparrot-train'\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń']\n"
     ]
    }
   ],
   "source": [
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "print(base_vocab) # 기본이 되는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=12500,\n",
    "    initial_alphabet = base_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'ĠH', 'ello', 'Wor', 'ld', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'hello', ')', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer(\"\"\"\\\n",
    "def HelloWorld():\n",
    "    print(\"hello)\n",
    "\"\"\").tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 갯수 : 35\n",
      "\"await\" is not in voc\n",
      "\"finally\" is not in voc\n",
      "\"nonlocal\" is not in voc\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "print(f\"\"\"\\\n",
    "키워드 갯수 : {len(keyword.kwlist)}\"\"\")\n",
    "for kw in keyword.kwlist:\n",
    "    if kw not in new_tokenizer.vocab:\n",
    "        print(f\"\\\"{kw}\\\" is not in voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=32768, # 8배수 => gpu 계산 효율적\n",
    "    initial_alphabet = base_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 갯수 : 35\n",
      "\"nonlocal\" is not in voc\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\\\n",
    "키워드 갯수 : {len(keyword.kwlist)}\"\"\")\n",
    "for kw in keyword.kwlist:\n",
    "    if kw not in new_tokenizer_larger.vocab:\n",
    "        print(f\"\\\"{kw}\\\" is not in voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tommyjin/tokenizer_from_codeparrot_dataset/commit/02ce052496e88c5e1f460497a5411c3d3bdd7928', commit_message='Upload tokenizer', commit_description='', oid='02ce052496e88c5e1f460497a5411c3d3bdd7928', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tommyjin/tokenizer_from_codeparrot_dataset', endpoint='https://huggingface.co', repo_type='model', repo_id='tommyjin/tokenizer_from_codeparrot_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_larger.push_to_hub(\"codeparrot\")\n",
    "new_tokenizer_larger.push_to_hub(\"codeparrot-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다양한 종류의 코드 생성 훈련 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 코잘 언어 모델링\n",
    "    - 코드 시작부분을 입력하고, 뒤를 입력해주는 작업\n",
    "        >input\n",
    "        >```python\n",
    "        >    def add_numbers(a,b):\n",
    "        >        \"add two numbers\"\n",
    "        >        return ____\n",
    "        >```\n",
    "        >decoder out\n",
    "        >```python\n",
    "        >    def add_numbers(a,b):\n",
    "        >        \"add two numbers\"\n",
    "        >        return a + b\n",
    "        >```\n",
    "2. 마스크드 언어 모델링(노이즈 제거)\n",
    "    - 입력 토큰중 일부가 마스킹 또는 변경\n",
    "        >input\n",
    "        >```python\n",
    "        >    class add_numbers(a,b):\n",
    "        >        \"add [MASK] numbers\"\n",
    "        >        return a+a\n",
    "        >```\n",
    "        >incoder out\n",
    "        >```python\n",
    "        >    def add_numbers(a,b):\n",
    "        >        \"add two numbers\"\n",
    "        >        return a + b\n",
    "        >```\n",
    "3. seq 2 seq 훈련\n",
    "    - 입력과 출력을 분리하여 입력에 따라 코드가 생성되는 일\n",
    "        >input\n",
    "        >```python\n",
    "        >        \"add two numbers\"\n",
    "        >```\n",
    "        >incoder out\n",
    "        >```python\n",
    "        >    def add_numbers(a,b):\n",
    "        >        return a + b\n",
    "        >```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tommyjin/codeparrot\")\n",
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size = len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 1529.6M 개\n"
     ]
    }
   ],
   "source": [
    "print(f'params {sum([p.numel() for p in model.parameters()])/1000**2:.1f}M 개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model/\" + \"codeparrot\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 111.0M 개\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tommyjin/codeparrot\")\n",
    "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size = len(tokenizer))\n",
    "model_small = AutoModelForCausalLM.from_config(config_small)\n",
    "print(f'params {sum([p.numel() for p in model_small.parameters()])/1000**2:.1f}M 개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.save_pretrained(\"model/\" + \"codeparrot\" + \"-small\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더 구축\n",
    "\n",
    "- 입력 글자수 = 시퀀스의 수 * 시퀀스 길이 * 글자별 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fbfd14c1624e1883181d496e921937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tommyjin/codeparrot-train\", split='train',\n",
    "                       streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:01<08:20,  1.00s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2599 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  5%|▌         | 27/500 [00:01<00:13, 34.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10개의 캐릭터 : import uni,\n",
      "        10개의 토큰  : ['import', 'Ġunittest', ',', 'Ġos', ',', 'Ġerrno', 'Ċ', 'from', 'Ġctypes', 'Ġimport']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 138.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6231516195736053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "examples, total_characters, total_tokens = 500, 0, 0\n",
    "for _, example in tqdm(zip(range(examples), iter(dataset)),total = examples):\n",
    "    total_characters += len(example['content'])\n",
    "    total_tokens += len(tokenizer(example['content']).tokens())\n",
    "\n",
    "    # 샘플 확인\n",
    "    if _ == 10 :\n",
    "        print(f'''\\\n",
    "        10개의 캐릭터 : {example['content'][:10]},\n",
    "        10개의 토큰  : {tokenizer(example['content']).tokens()[:10]}''')\n",
    "    \n",
    "character_per_token = total_characters / total_tokens\n",
    "print(character_per_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍을 위한 버퍼 채우기(일정한 크기만큼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
    "                 num_of_sequences=1024, chars_per_token = 3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        iteration = iter(self.dataset)\n",
    "        more_sample = True\n",
    "        while more_sample:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m = f\"버퍼 채우는중: {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m = f\"버퍼 채우기완: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iteration)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iteration = iter(self.dataset)\n",
    "\n",
    "            all_token_ids = []\n",
    "            tokenizer_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenizer_input in tokenizer_inputs['input_ids']:\n",
    "                all_token_ids.extend(tokenizer_input + [self.concat_token_id])\n",
    "\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i:i+self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n",
    "                                                num_of_sequences=10)\n",
    "\n",
    "dataset_iteration = iter(constant_length_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버퍼 채우기완: 0<36864\n",
      "버퍼 채우기완: 1804<36864\n",
      "버퍼 채우는중: 44799>=36864\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(b) for _, b in zip(range(5), dataset_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1024, 1024, 1024, 1024, 1024]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wnadb 와 그라디언트 어큐뮬레이션을\n",
    "\n",
    "- wandb 를 위한 사전 준비\n",
    "  - login with wandb\n",
    "    ```bash\n",
    "    wandb login\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tommyjin2894-personal/large-dataset-training/runs/tlnkbf38?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fedfccdd690>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "# 1. wandb 설정\n",
    "wandb.init(\n",
    "    project=\"large-dataset-training\",\n",
    "    config={\n",
    "        \"model_name\": \"codeparrot-small\",\n",
    "        \"train_batch_size\": 8,\n",
    "        \"valid_batch_size\": 8,\n",
    "        \"accumulation_steps\": 16,\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"max_length\": 128,\n",
    "        \"seq_length\": 1024,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"num_warmup_steps\": 750,\n",
    "        \"max_train_steps\": 50,\n",
    "        \"save_checkpoint_steps\": 200,\n",
    "        \"shuffle_buffer\": 10,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "\n",
    "# 모델 및 토크나이저 설정\n",
    "model_name = wandb.config[\"model_name\"]\n",
    "model = AutoModelForCausalLM.from_pretrained('tommyjin/'+model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('tommyjin/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e97a4dca5443d7b1b278df1074ff00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 스트리밍\n",
    "dataset = load_dataset(\"tommyjin/codeparrot-train\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess(batch):\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"content\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=wandb.config[\"max_length\"],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(32768, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr=wandb.config[\"learning_rate\"],\n",
    "                  weight_decay=wandb.config[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Precision Training\n",
    "scaler = torch.amp.GradScaler(device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb를 위한 설정\n",
    "wandb.watch(model, log=\"all\", log_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  훈련 args\n",
    "accumulation_steps = wandb.config[\"accumulation_steps\"]\n",
    "batch_size = wandb.config[\"train_batch_size\"]\n",
    "epochs = wandb.config[\"epochs\"]\n",
    "save_checkpoint_steps = wandb.config[\"save_checkpoint_steps\"]\n",
    "wandb_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, example in enumerate(dataset):\n",
    "        processed = preprocess(example)\n",
    "        batch[\"input_ids\"].append(processed[\"input_ids\"])\n",
    "        batch[\"attention_mask\"].append(processed[\"attention_mask\"])\n",
    "\n",
    "        # 배치 크기가 지정된 값에 도달하면 학습 진행\n",
    "        if len(batch[\"input_ids\"]) == batch_size:\n",
    "            input_ids = torch.stack(batch[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.stack(batch[\"attention_mask\"]).to(device)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):  # Mixed Precision Training\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # 그라디언트 누적\n",
    "            loss = loss / accumulation_steps  # 손실을 어큐뮬레이션 스텝 수로 나누어 평균 처리\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # accumulation_steps 만큼 그라디언트가 누적되면 파라미터 업데이트\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # wandb 로깅\n",
    "            if (step + 1) % wandb_interval == 0:\n",
    "                wandb.log({\"loss\": loss.item(), \"step\": step + 1})\n",
    "                print(f\"Step {step + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "            # 배치 초기화\n",
    "            batch = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "    # 에포크 종료 후 wandb에 에포크 단위 로깅\n",
    "    wandb.log({\"epoch\": epoch + 1})\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    if step % save_checkpoint_steps == 0:\n",
    "        # 디렉터리 경로 설정\n",
    "        checkpoint_path = f\"/mnt/e/ai_career/ai_study_Transformer/codeparrot-small/checkpoint_epoch_{step}\"\n",
    "\n",
    "        # 모델과 토크나이저 저장\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "    if (epoch + 1) % save_checkpoint_steps == 0:\n",
    "        wandb.save(f\"checkpoint_epoch_{epoch + 1}/*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tommyjin/codeparrot-small/tokenizer_config.json',\n",
       " 'tommyjin/codeparrot-small/special_tokens_map.json',\n",
       " 'tommyjin/codeparrot-small/vocab.json',\n",
       " 'tommyjin/codeparrot-small/merges.txt',\n",
       " 'tommyjin/codeparrot-small/added_tokens.json',\n",
       " 'tommyjin/codeparrot-small/tokenizer.json')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"tommyjin/codeparrot-small\")\n",
    "tokenizer.save_pretrained(\"tommyjin/codeparrot-small\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "wandb.save(\"codeparrot-small/*\")\n",
    "\n",
    "print(\"Training completed and model saved.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잘훈련된 모델 테스트 from `transformersbook/codeparrot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = 'transformersbook/codeparrot'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, cache_dir = \"./cache_dir\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir = \"./cache_dir\")\n",
    "generation = pipeline('text-generation',\n",
    "                      model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "predicted = generation('''# this code is most nice code ever\n",
    "def lit_code(''',\n",
    "temperature = 1.3,\n",
    "top_k = 15,\n",
    "do_sample = True,\n",
    "num_beams = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this code is most nice code ever\n",
      "def lit_code(code):\n",
      "\tif code[0:2]=='#':\n",
      "\t\treturn '[[%02x,%s]]' % (chr(code[2+2)%31,code\n"
     ]
    }
   ],
   "source": [
    "print(predicted[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this code is most nice code ever\n",
      "def lit_code(code):\n",
      "\tif code[0:2]=='#':\n",
      "\t\treturn '[[%02x,%s]]' % (chr(code[2+2)%31,code\n"
     ]
    }
   ],
   "source": [
    "print(predicted[0]['generated_text'].split(\"\\n\\n\")[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
