{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 매커니즘\n",
    "\n",
    "### 1. RNN 구조의 인코더 디코더의 문제점\n",
    "- 최종 은닉상태의 정보의 병목현상\n",
    "- 시퀀스의 최근데이터의 가중화.\n",
    "- 시퀀스의 초기데이터의 희미해짐.\n",
    "- 순차 입력, 순차 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. 인코더 디코더에서의 어텐션 매커니즘\n",
    "1. 디코더가 모든스텝의 인코더에 가중치(어텐션) 을 할당 <br>\n",
    "\n",
    "    | 단어          | 은닉 상태 | 어텐션 가중치 |\n",
    "    |---|---|---|\n",
    "    | I             | t1        | 0.2            |\n",
    "    | am            | t2        | 0.1            |\n",
    "    | going         | t3        | 0.3            |\n",
    "    | home          | t4        | 0.4            |\n",
    "\n",
    "    **즉 인코더의 모든 스텝에서 은닉상태를 출력 한다.**\n",
    "    <br>\n",
    "2. 어텐션 가중치를 통한 출력 계산 : $Output = (0.2 * t1) + (0.1 * t2) + (0.3 * t3) + (0.4 * t4)$\n",
    "    \n",
    "    \n",
    "3. 최종 출력 생성 : **\"나는\"**\n",
    ">셀프 어텐션 : <br>\n",
    ">모든 상태에서 어텐션을 작동 시킨다.<br>\n",
    ">NLP(Natural Language Processing) 에서의 핵심 기술<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ULMFiT(Universal Language Model Fine-tuning)\n",
    "- NLP 에 특화된 전이 학습 및 파인 튜닝(다양한 작업에서 사전 훈련된 LSTM 모델을 이용)\n",
    "- ULMFiT 의 세 단계\n",
    "    - Language Modeling(Pretraining) : <br>이전 단어를 기반으로 다음 단어 예측, <br>라벨링 데이터가 필요 없다.\n",
    "    - adjust Domain : Pretrain 이후 도메인 내에 말뭉치 적응 시키기\n",
    "    - fine tuning : 언어 모델의 타겟작업을 분류층과 함께 미세 튜닝"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
