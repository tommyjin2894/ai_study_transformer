{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN/DailyMail Dataset 을 이용한 summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': Value(dtype='string', id=None),\n",
       " 'highlights': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][10][\"article\"][:2000]\n",
    "summarys = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk, sent tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/tommy/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/usr/local/share/nltk_data')\n",
    "\n",
    "# 'punkt' 다운로드 및 찾기\n",
    "nltk.download('popular')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약 모델의 baseline\n",
    "- 주로 요약 모델의 baseline **첫 문장 3개**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WASHINGTON (CNN) -- As he awaits a crucial progress report on Iraq, President Bush will try to put a twist on comparisons of the war to Vietnam by invoking the historical lessons of that conflict to argue against pulling out.',\n",
       " 'President Bush pauses Tuesday during a news conference at the  North American Leaders summit in Canada.',\n",
       " 'On Wednesday in Kansas City, Missouri, Bush will tell members of the Veterans of Foreign Wars that \"then, as now, people argued that the real problem was America\\'s presence and that if we would just withdraw, the killing would end,\" according to speech excerpts released Tuesday by the White House.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset[\"train\"][10][\"article\"][:2000]\n",
    "\n",
    "sent_tokenize(test)[:3] # sentance spliting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str + \"TL;DR\"\n",
    "- too long didn't read\n",
    "- 한국어 버젼으로 \"3 줄 요약좀..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt2 for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 19:12:45.975179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733652766.104591    1012 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733652766.143635    1012 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 19:12:46.471085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "query = test + \"\\nTL;DR\\n\"\n",
    "pip_out = pipe(query, max_length = 512, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarys[\"gpt2\"] = pip_out[0][\"generated_text\"].split(\"\\nTL;DR\\n\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
    "pip_out = pipe(test)\n",
    "summarys[\"t5\"]=pip_out[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': \"To be fair, you can't compare this war to prior conflicts in east Asia in the sense that there's a difference between military intervention and war. Also, the president's speech excerpts are not necessarily a full transcript. The White House says it will release the full remarks sometime in the next 2-3 days.\",\n",
       " 't5': 'president bush will try to put a twist on comparisons of the war to Vietnam . he\\'ll tell veterans of foreign war that \"the real problem was America\\'s presence\" \"the price of america\\'s withdrawal was paid by millions of innocent citizens\" speech excerpts released by the white house .'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=\"cuda\")\n",
    "pip_out = pipe(test)\n",
    "summarys[\"bart\"]=pip_out[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': \"To be fair, you can't compare this war to prior conflicts in east Asia in the sense that there's a difference between military intervention and war. Also, the president's speech excerpts are not necessarily a full transcript. The White House says it will release the full remarks sometime in the next 2-3 days.\",\n",
       " 't5': 'president bush will try to put a twist on comparisons of the war to Vietnam . he\\'ll tell veterans of foreign war that \"the real problem was America\\'s presence\" \"the price of america\\'s withdrawal was paid by millions of innocent citizens\" speech excerpts released by the white house .',\n",
       " 'bart': \"President Bush to tell Veterans of Foreign Wars about Vietnam War. He will argue withdrawal from Vietnam emboldened terrorists, he will say. Bush will cite Osama bin Laden's quote that U.S. would rise against Iraq war. Senate Majority Leader Harry Reid says comparison ignores difference between wars.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEGASUS for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", device=\"cuda\")\n",
    "pip_out = pipe(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarys[\"pegasus\"]=pip_out[0][\"summary_text\"].replace('<n>','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델별 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[grund truth]\n",
      "London (CNN)A 19-year-old man was charged Wednesday with terror offenses after he was arrested as he returned to Britain from Turkey, London's Metropolitan Police said. Yahya Rashid, a UK national from northwest London, was detained at Luton airport on Tuesday after he arrived on a flight from Istanbul, police said. He's been charged with engaging in conduct in preparation of acts of terrorism, and with engaging in conduct with the intention of assisting others to commit acts of terrorism. Both charges relate to the period between November 1 and March 31. Rashid is due to appear in Westminster Magistrates' Court on Wednesday, police said. CNN's Lindsay Isaac contributed to this report. \n",
      "\n",
      "[gpt2]\n",
      "To be fair, you can't compare this war to prior conflicts in east Asia in the sense that there's a difference between military intervention and war. Also, the president's speech excerpts are not necessarily a full transcript. The White House says it will release the full remarks sometime in the next 2-3 days.\n",
      "\n",
      "[t5]\n",
      "president bush will try to put a twist on comparisons of the war to Vietnam . he'll tell veterans of foreign war that \"the real problem was America's presence\" \"the price of america's withdrawal was paid by millions of innocent citizens\" speech excerpts released by the white house .\n",
      "\n",
      "[bart]\n",
      "President Bush to tell Veterans of Foreign Wars about Vietnam War. He will argue withdrawal from Vietnam emboldened terrorists, he will say. Bush will cite Osama bin Laden's quote that U.S. would rise against Iraq war. Senate Majority Leader Harry Reid says comparison ignores difference between wars.\n",
      "\n",
      "[pegasus]\n",
      "President Bush to speak to Veterans of Foreign Wars on Wednesday .Bush to say withdrawing from Vietnam emboldened terrorists by compromising U.S. credibility .Speech excerpts released Tuesday by the White House .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[grund truth]\")\n",
    "print(dataset[\"test\"][10][\"article\"],'\\n')\n",
    "\n",
    "for k,v in summarys.items():\n",
    "    print(f\"[{k}]\")\n",
    "    print(v, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가 (BLEU:Bilingual Evaluation Understudy Score)\n",
    "- 참조 텍스트와 얼마나 많이 정렬되었는지, 단어 or n-gram 을 체크 (:정밀도 에서 비롯된 방법)\n",
    "    - 수정 정밀도($p_n$) 공식\n",
    "        $$ p_n = \\frac{\\sum_{\\text{n-gram} \\in snt'}Count_{\\text{clip}}(\\text{n-gram})}{\\sum_{\\text{n-gram} \\in snt}Count(\\text{n-gram})}$$\n",
    "        에서<br>\n",
    "            $snt$ : **gen** 된 (후보)candidate 문장 <br>\n",
    "            $snt'$ : 참조(reference) 문장 <br>\n",
    "            $\\text{n-gram}$ : 연속된 단어의 조합 <br>\n",
    "            $Count_{\\text{clip}}(\\text{n-gram})$ : 클리핑 된 $\\text{n-gram}$ 의 수(겹치는 $\\text{n-gram}$)<br>\n",
    "            $Count(\\text{n-gram})$ : 생성된 문장의 $\\text{n-gram}$ 갯수 <br>\n",
    "    - 예시 (1-gram)\n",
    "        - ref = \"the cat is on the mat\"\n",
    "        - gen = \"<u>the</u> the the the <u>the</u> the\"\n",
    "\n",
    "        $$  P_{n} = \\frac{\\text{문장 일치수}}{\\text{생성된 총 단어 수}} =  \\frac{2}{6}$$\n",
    "\n",
    "    - 재현 률을 고려한 항 **BR(brevity penalty)** : 즉 생성된 문장이 참조 문장보다 짧은지의 척도, 생성된 문장이 길면 $BR = 1$ 이다\n",
    "        $$BR = \\min(1, e^{1-l_{\\text{ref}}/l_{\\text{gen}}})$$\n",
    "        - 결과 길이를 고려한 항,\n",
    "        - 번역 길이가 짧은경우 P 값이 상승하는 경향이 있어\n",
    "        - 길이가 짧을 경우 패널티를 준다.\n",
    "        - 생성된 길이가 참조 문장보다 길면 1로 반환 한다.\n",
    "    - BLEU-N 점수 계산\n",
    "        - 수정 정밀도의 기하평균 : $$\\left(\\prod_{n=1}^{N} p_n\\right)^{1/N} == \\exp{\\left(\\frac{1}{N}\\prod_{n=1}^{N} \\log p_n \\right)}$$\n",
    "        - BLEU-N 점수 계산 : $$\\text{BLEU-N} = \\text{BR} \\times \\left(\\prod_{n=1}^{N} p_n\\right)^{1/N}$$\n",
    "- 단점 : \n",
    "    - **동의어를** 고려하지 않음\n",
    "    - **토큰화된 텍스트**를 기대한다. <br>\n",
    "        ex.<br>\n",
    "        reference : [I'm a doctor] -> ['I',\"'\",'m',' a','doctor']<br>\n",
    "        candidate : [I am a doctor] -> ['I',' am',' a','doctor']<br>\n",
    "        에서 I'm 과 I am 은 동의어로 치부하지 않게 된다. 또한 토큰 길이도 다르게 될 수 있어 이에 대한 패널티를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{BLEU-N} = \\text{BR} \\times \\left(\\prod_{n=1}^{N} p_n\\right)^{1/N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.3333333333333333, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "# 1-gram test\n",
    "import evaluate\n",
    "bleu= evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "ref = \"the cat is on the mat\"\n",
    "can = \"the the the the the the\"\n",
    "\n",
    "b_result =bleu.compute(predictions=[can], references=[ref])\n",
    "print(b_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-N              (bleu                ):0.4264014327112209\n",
      "p                   (precisions          ):[0.5, 0.36363636363636365]\n",
      "BP                  (brevity_penalty     ):1.0\n",
      "BR                  (length_ratio        ):2.0\n",
      "len_gen             (translation_length  ):12\n",
      "len_ref             (reference_length    ):6\n",
      "\n",
      "생성된 문장의 길이가 참조 문장보다 길다.\n",
      "BR => 1 로 계산된다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#n-gram test\n",
    "bleu= evaluate.load(\"bleu\")\n",
    "\n",
    "ref = \"the cat is on the mat\"\n",
    "# can = \"the the the the the the\"\n",
    "can_for_2_gram = \"the cat the the the is on the mat mat mat mat\"\n",
    "\n",
    "b_result =bleu.compute(predictions=[can_for_2_gram], references=[ref], max_order = 2) # max_order 로 n-gram 을 정해준다.\n",
    "for f,(k,v) in zip([\"BLEU-N\",\"p\",\"BP\", \"BR\",\"len_gen\",\"len_ref\"],b_result.items()):\n",
    "    print(f\"{f:20s}({k:20s}):{v}\")\n",
    "\n",
    "print(\"\"\"\n",
    "생성된 문장의 길이가 참조 문장보다 길다.\n",
    "BR => 1 로 계산된다\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 25.211936184349828,\n",
       " 'counts': [6, 4, 2, 1],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [50.0, 36.36363636363637, 20.0, 11.11111111111111],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 6}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sacre bleu : 토큰화 단계를 내제화\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "sb_result = sacrebleu.compute(predictions=[can_for_2_gram], references=[ref])\n",
    "sb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가 (ROUGE:Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- ROUGE-N Recall(참조 문장) 기반 $$ \\text{ROUGE-N} = \\frac{\\sum_{\\text{n-gram} \\in \\text{ref}} \\min(\\text{Count}_{\\text{gen}}(\\text{n-gram}), \\text{Count}_{\\text{ref}}(\\text{n-gram}))}{\\sum_{\\text{n-gram} \\in \\text{ref}} \\text{Count}_{\\text{ref}}(\\text{n-gram})}$$\n",
    "    $$ \\frac{\\text{모든 참조 문장의 <n-gram과 생성문장의 n-gram 중 최솟값 들>의 합}}{\\text{모든 참조 문장의 <n-gram 의 경우의 수>의 합}}$$\n",
    "\n",
    "- ROUGE-N Precision (생성된 문장) 기반 $$ \\text{ROUGE-N} = \\frac{\\sum_{\\text{n-gram} \\in \\text{gen}} \\min(\\text{Count}_{\\text{gen}}(\\text{n-gram}), \\text{Count}_{\\text{ref}}(\\text{n-gram}))}{\\sum_{\\text{n-gram} \\in \\text{gen}} \\text{Count}_{\\text{gen}}(\\text{n-gram})}$$\n",
    "\n",
    "- ROUGE-N f-1 score $$ \\text{f1-score} = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{preciaion} + \\text{recall}}$$\n",
    "\n",
    "    > BLUE 와의 비교 $$\\text{BLEU-N} = \\min(1, e^{1-l_{\\text{ref}}/l_{\\text{gen}}}) \\times \\prod_{n=1}^{N} \\left( \\frac{\\sum_{\\text{n-gram} \\in snt'}Count_{\\text{clip}}(\\text{n-gram})}{\\sum_{\\text{n-gram} \\in snt}Count(\\text{n-gram})} \\right)^{1/N}$$\n",
    "\n",
    "- ROUGE-L\n",
    "    - LCS 점수 (Longest Common Subsequence : 가장 긴 공통 시퀀스)\n",
    "        - 문자열 기준 에서 비교가 가능 (\"<u>appl</u>e\" and \"<u>appl</u>ication\")\n",
    "        - $\\text{X} = \\text{*appl*e}$ 와 $\\text{Y} = \\text{*appl*ication}$ 에서\n",
    "          $$\\text{LCS(X,Y)} = 4$$\n",
    "          $$R_{\\text{LCS}} = \\frac{\\text{LCS(X,Y)}}{m} = \\frac{4}{5}$$\n",
    "          $$P_{\\text{LCS}} = \\frac{\\text{LCS(X,Y)}}{n} = \\frac{4}{11}$$\n",
    "          $$F_{\\text{LCS}} = \\frac{(1 + \\beta^2) R_{\\text{LCS}} P_{\\text{LCS}}}{R_{\\text{LCS}} + P_{\\text{LCS}}}$$\n",
    "            - 만약 $\\beta = 1$ 일때 기존 **f1-score** 공식이랑 동일 <br>(`transformer` 는 $\\beta = 1$ 로 계산)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- meteor 및 rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE\n",
      "{'rouge1': 0.3333333333333333, 'rouge2': 0.0, 'rougeL': 0.3333333333333333, 'rougeLsum': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "r_result = rouge.compute(predictions=[can], references=[ref])\n",
    "print(\"ROUGE\")\n",
    "print(r_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.160920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107784</td>\n",
       "      <td>0.107784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>0.080537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2    rougeL  rougeLsum\n",
       "gpt2     0.160920  0.000000  0.080460   0.080460\n",
       "t5       0.179641  0.000000  0.107784   0.107784\n",
       "bart     0.096386  0.000000  0.072289   0.072289\n",
       "pegasus  0.134228  0.013605  0.080537   0.080537"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'전반적으로 pegasus 의 성능이 좋다'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_scores = {}\n",
    "\n",
    "for k,v in summarys.items():\n",
    "    scores = rouge.compute(references=[dataset[\"test\"][10][\"article\"]],predictions=[v])\n",
    "    all_scores[k] = scores\n",
    "\n",
    "display(pd.DataFrame(all_scores).T)\n",
    "\"전반적으로 pegasus 의 성능이 좋다\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 meteor 스코어 $$\\text{METEOR} = \\frac{10 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\times \\left(1 - \\frac{\\text{Penalty}}{\\text{len}} \\right)$$\n",
    "\n",
    "    여기서\n",
    "    $$Precision = \\frac{\\text{LCS}(X,Y)}{m}$$\n",
    "    $$Recall = \\frac{\\text{LCS}(X,Y)}{n}$$\n",
    "    $$\\text{Penalty} = n_{\\text{chunk}} - 1$$\n",
    "    - chunk 의 기준\n",
    "        - 생성문, 참조문 단어 순서 일치 부분\n",
    "        - 완전히 일치하는 경우 패널티 = 0\n",
    "        - 일치하는 chunk 가 많아 질 수록 패널티 증가\n",
    "        - 즉 순서차이가 큰 문장에서 패널티 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************************************************************************************************************************************\n",
      "{'meteor': 0.16666666666666666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/tommy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "metero = evaluate.load(\"meteor\")\n",
    "m_result = metero.compute(predictions=[can], references=[ref])\n",
    "print(\"*\"*200)\n",
    "print(m_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
