{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션\n",
    "\n",
    "- 어텐션층의 공간복잡도는 약 $n^2$ 이다<br>\n",
    "$n$ : 시퀀스 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산이 중요하다.\n",
    "- 규모의 법칙\n",
    "    - N, C, D 를 동시에 증가시키는 것이 더 나은 모델을 만드는 데 생산적이다.\n",
    "        - N: 모델 크기\n",
    "        - C: 컴퓨팅 예산\n",
    "        - D: 데이터 셋 크기\n",
    "\n",
    "- 거듭 제곱 법칙(power law)\n",
    "    - 언어 모델의 성능은 거듭 제곱 법칙을 따른다.\n",
    "        - 테스트 손실 $L(X) \\sim \\left( \\frac{1}{X} \\right)^{\\alpha}$\n",
    "\n",
    "- 샘플 효율성\n",
    "    - 대규모 모델은 적은 훈련횟수로도 소규모 모델과 동일한 성능을 낸다.\n",
    "        - 이미지, 비디오, 수학 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 규모확장의 어려움\n",
    "- 인프라\n",
    "    - GPU 가 여럿, 노드가 수백 수천 개를 관리하는 일은 쉽지 않다.\n",
    "    - 대규모 분산 을 능숙하게 해야한다.\n",
    "- 비용\n",
    "    - GPT-3 모델 하나 훈련시에 수백만 달러가 들기도 한다.\n",
    "    - GPT-4 훈련에 1억달러(약 1400억) 이상이 들었다\n",
    "- 고품질의 데이터셋\n",
    "    - 전처리의 어려움\n",
    "    - 고품질의 데이터셋인지 확인의 어려움\n",
    "    - 성차별, 편향 제어 방법 모색\n",
    "- 모델 평가\n",
    "    - 언어 모델 평가에 대한 시간과 자원\n",
    "    - 유해성 검증\n",
    "- 배포\n",
    "    - 대규모 모델을 서빙하는데 어려움\n",
    "    - 양자, 가지치기, 정제등의 모델 압축 및 경량화 기술이 있지만 충분치 않다.\n",
    "        - Openai Api, Accelerated Inference Api\n",
    "\n",
    "- BigScience, EleutherAI\n",
    "    - 오픈소스 연구 단체 들\n",
    "    - 여러 오픈소스 모델을 훈련하였고 배포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다양한 어텐션 연구방향 https://arxiv.org/abs/2106.04554 (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 희소 어텐션\n",
    "    - ![sparse attn](images/11_01.png)\n",
    "    - ![model](images/11_02.png)\n",
    "- 선형 어텐션\n",
    "    - ![linear attn](images/11_03.png)\n",
    "        - 기존 식 : $$ \\text{softmax}(\\frac{Q \\times K^T}{\\sqrt d_k}) \\times V$$\n",
    "        - 선형 식 : $$ \\phi(Q) \\times  (\\phi(K)^T \\times V)$$ 여기서 $\\phi$ 는 특성 변환 함수\n",
    "            - 특성 변환 함수 예시(피쳐 매핑) :\n",
    "                1. 가우시안 커널 근사 $\\phi(x) = e^x$\n",
    "                2. ReLU 변형 $\\phi(x) = \\text{max}(x,0)$\n",
    "    - 이러한 방식으로 복잡성이 $n^2$ -> $n$\n",
    "- 쿼리 프로토타이핑과 모델 압축\n",
    "    - ![qp_mc](images/11_04.png)\n",
    "- 로우랭크 self attention\n",
    "\n",
    "- 이외의 다양한 논문\n",
    "    - [A SURVEY ON TRANSFORMERS IN NLP WITH FOCUS ON EFFICIENCY](https://arxiv.org/pdf/2406.16893)\n",
    "    - [A Survey on Transformer Compression](https://arxiv.org/pdf/2402.05964)\n",
    "    - [AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION](https://arxiv.org/pdf/2306.00978)\n",
    "    - [DIFFERENTIAL TRANSFORMER](https://arxiv.org/pdf/2410.05258)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비전 트랜스 포머\n",
    "- iGPT (image GPT)\n",
    "    - 이미지의 픽셀을 시퀀스로 보고 GPT 모델구조와 자기회귀 를 이용해 훈련\n",
    "- ViT\n",
    "    - ![11_04](images/11_05.png)\n",
    "    - 이미지의 BERT 스타일의 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테이블 데이터\n",
    "- 쿼리와 결합한 트랜스 포머 아키텍쳐\n",
    "    - ![11_04](images/11_06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 멀티 모달 트랜스 포머\n",
    "- 스피치 투 텍스트\n",
    "    - ![11_07](images/11_07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비전 텍스트 모델\n",
    "    - VQA\n",
    "    - LayoutLM\n",
    "    - DALLE\n",
    "    - CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test for my self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[aya-expanse-8b](https://huggingface.co/CohereForAI/aya-expanse-8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"hf_key_token.json\") as j:\n",
    "    key = json.load(j)['hf_write_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61b14a8c0064ef79414aa9f043af6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token=key)\n",
    "\n",
    "model_id = \"CohereForAI/aya-expanse-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Format the message with the chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=100, \n",
    "    do_sample=True, \n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0])\n",
    "print(gen_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
